{"cells":[{"cell_type":"markdown","source":["### Metadata-Driven Ingestion Framework \n#### Data Ingestion: Get Schemas\nConnect to sink instance and get the schema of the columns of parquet file. Validate the result and send it to Azure Data Factory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abdf5cf5-bf43-49e7-9493-50575e9547c0"}}},{"cell_type":"markdown","source":["### IMPORTANT!\n#### Configuration for testing and debug\nChange the value of \"testing=False\" for production environment.\nChange the value of debug variables to see or hide prints with information."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1e7b807-8b16-46c1-947e-b1a4cc7a0a0a"}}},{"cell_type":"markdown","source":["#### Declaration of variables and execution of functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f44148e-e1a2-4af3-8886-9fd53afc7815"}}},{"cell_type":"code","source":["#Class that will evaluate cast for specific column of each datatype \n\nclass DTypecheck: # it works for pyspark dataframe only\n\n    def __init__(self, dataframe):\n        #when class is initialized store dataframe to shorter names\n        import dateutil\n        self.df = dataframe\n        self.acceptance = 0.7\n\n    @classmethod\n    def from_pdf(cls,pdf): #this is if we have a pandas dataframe it should run like DTypecheck.from_pdf(nameofpandasdf).get_schema()\n        df=spark.createDataFrame(pdf) \n        return cls(df)\n    \n    #function to format schema\n    @staticmethod\n    def set_format_schema(a_jsonschema):\n        schema = []\n        try:\n            for item in a_jsonschema:\n                if \"varchar\" in item['type'].lower():\n                    new_schema = f\"{item['name']} STRING\"\n                elif item['type'].lower() == 'int':\n                    new_schema = f\"{item['name']} INTEGER\"\n                elif item['type'].lower() == 'bigint':\n                    new_schema = f\"{item['name']} LONG\"\n                elif item['type'].lower() == 'decimal(18,4)':\n                    new_schema = f\"{item['name']} DECIMAL\"\n                elif item['type'].lower() == 'datetime':\n                    new_schema = f\"{item['name']} TIMESTAMP\"\n                elif item['type'].lower() == 'bit':\n                    new_schema = f\"{item['name']} BOOLEAN\"\n                else:\n                    new_schema = f\"{item['name']} {item['type']}\"\n                schema.append(new_schema)\n            schema = ', '.join(schema)\n\n            return schema\n        except Exception as exp:\n            print(f\"Error set_format_schema: {exp}\")\n\n    #functions to cast schemas\n    def bit(self,column2evaluate): #function to evaluate if values are boolean, 1st function to evaluate\n\n        self.col=column2evaluate\n        df2=self.df\n        try:\n            #if 1s or 0s replace them with none bc it will think that it is a True or False when casting\n            df2 = df2.withColumn(self.col, when(df2[self.col] == '1', None).otherwise(df2[self.col]))\n            df2 = df2.withColumn(self.col, when(df2[self.col] == '0', None).otherwise(df2[self.col]))\n\n            df3 = df2.withColumn(self.col, df2[self.col].cast(BooleanType())) #perform casting\n            if df3.filter(df3[self.col].isNull()).count()>=self.acceptance*(df3.count()) or df2.where(f'{self.col} rlike \"[0-9]\"').count()>0: #if more than 70% are null values then casting failed and col is not bit\n                return 0\n            else:\n                return 1\n        except:\n            return 0\n\n    def integer(self,column2evaluate): # use this function to validate for int if and only if bit failed, 2nd funct to evaluate \n\n        self.col=column2evaluate\n        #check for null values to avoid counting more nulls\n        df2=self.df\n        try:\n            df3 = df2.withColumn(self.col, round(df2[self.col]).cast(IntegerType())) #round up to make sure first they are numbers\n            if df3.filter(df3[self.col].isNull()).count()>=self.acceptance*(df3.count()): #if more than 70% are nulls then its sure not an integer\n                return 0\n            else:\n                return 1\n        except:\n            return 0\n\n    def decimal(self,column2evaluate): #this funct should only be called if integer returned 1, 3rd function to evaluate\n        self.col = column2evaluate\n        df2 = self.df\n        try:\n            df3 = df2.withColumn(self.col, (df2[self.col]).cast(\"decimal(18,4)\")) #round up to make sure first they are numbers\n            if df3.filter(df3[self.col].isNull()).count()>=self.acceptance*(df3.count()): #if more than 70% are nulls then its sure not an integer\n                return 0\n            else:\n                if (self.df.filter(self.df[self.col].contains('.'))).count()==0:\n                    return 0\n                else:\n                    return 1\n        except:\n            return 0 \n\n    def biginteger(self,column2evaluate): #this function should be called ig integer is validated and decimal is not 4th funct to evaluate\n\n        self.col=column2evaluate\n        df3 = self.df.withColumn(self.col, round(self.df[self.col]).cast(IntegerType())) #round up to make sure first they are numbers\n        if df3.where(df3[self.col]==2147483647).count()>0: #check if there is a value higher than the largest int it will set it to max int\n            return 1\n        else:\n            return 0\n\n    def date(self,column2evaluate): #if integer returned 0, bit must've returned 0 too, so we try date 5th function to evaluate\n\n        import dateutil\n        self.col=column2evaluate\n        cont=0\n        for el in self.df.select(self.col).collect(): #iterate over each value of the columns\n            try: \n                dateutil.parser.parse(el[0]) #try to parse it to date\n            except:\n                cont+=1 #if you cant parse it to date store the issue \n        if cont>=self.acceptance*self.df.select(self.col).count():#if more than 70% of the data cant be converted to datetime this col shouldn't be datetime\n            return 0\n        else:\n            return 1\n\n    def datetime(self,column2evaluate): #this func should only be called after validating that date is 1 \n\n        import dateutil\n        self.col=column2evaluate\n        contv=0\n        for el in self.df.select(self.col).collect():\n            try:\n                val=(dateutil.parser.parse(el[0])) #we know that data can be parsed since date returned 1\n                try:\n                    if val.hour>0 or val.minute>0 or (self.df.withColumn(self.col, col(self.col).cast('timestamp'))): #if at least 1 value has hour or minute set to more than 1 its datetime\n                        contv+=1\n                except:\n                    pass\n            except:#if it cant be parsed it is an empty string so we just ignore it\n                pass\n        if contv>0: #if we registered that at least one element that is datetime has hours then we use datetime not date\n            return 1\n        else:\n            return 0\n\n    def binary(self,column2evaluate):\n        self.col=column2evaluate\n        df2=self.df    \n        val=0\n        try:\n            df3 = df2.withColumn(self.col, df2[self.col].cast(BinaryType())) #perform casting\n            for c in df3.select(self.col).collect():\n                if c[0]==None:\n                    pass\n                else:\n                    c[0].decode()\n        except:\n            val+=1\n    \n        if val>0:\n            return 1\n        else:\n            return 0\n\n    \n    def maxvarchar(self,column2evaluate): #we should use this function only after validating it it is not anything else \n        #this function will be evaluated second to last\n        self.col=column2evaluate\n        if self.df.where(length(col(self.col)) >255).count()>0: #if we have at least one value higher than 255 use var char max\n            return 1\n        else:\n            return 0\n\n    def varchar(self,column2evaluate):\n        #this function should be evaluated last\n        self.col=column2evaluate\n        if self.maxvarchar(self.col)==0:\n            return 1\n\n    def get_schema(self):\n\n        schema = []\n        cols = self.df.columns\n\n        for n,c in enumerate(cols, 1): #Main logic to validate schemas for each column\n\n            data = {}\n            data['id'] = str(n)\n            data['name'] = c\n            #check if column is boolean\n            if self.bit(c) == 1:\n                data['type']='Bit'\n            \n\n            elif self.decimal(c) == 1:\n                    data['type'] = 'Decimal(18,4)'\n            elif self.integer(c) == 1: #now we know this column is a number so we can check for .\n\n                if self.biginteger(c) == 1: #if it is not a decimal it still may be a big integer\n                    data['type']='Bigint'\n                else:\n                    data['type']='Int'\n            \n            elif self.date(c) == 1: #if it is not an integer check it can be parsed to date \n                if self.datetime(c) == 1:\n                    data['type'] = 'Datetime'\n\n                else:\n                    data['type'] = 'Date'\n\n            else: #it should definitely be a string\n                if self.binary(c) == 1:\n                    data['type']='Binary'          \n                \n                elif self.maxvarchar(c)==1:\n                    data['type']='Varchar (8000)'\n                \n                       \n                else: #if its not any of those it should be this one\n                    data['type']='Varchar (255)'\n                        \n            schema.append(data)\n\n        return schema\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1012ef7-be2a-4a03-bde3-9b7478a9ded3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#jsonschema\n#new_schema=DTypecheck(df).set_format_schema(jsonschema)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bca5b98a-c80a-4d64-b3e4-765a15e492bc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DI_02_Get_SchemaTest","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{"ContainerName":{"nuid":"50d041ef-4d7a-4bfb-a6fd-3197d256c991","currentValue":"","widgetInfo":{"widgetType":"text","name":"ContainerName","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"FilePath":{"nuid":"836ca63f-3f03-489c-83de-44931b1ad54d","currentValue":"Converted/ADLS//TestAlexDT/Audi/2022/05/06/","widgetInfo":{"widgetType":"text","name":"FilePath","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"File":{"nuid":"1302a805-e816-4697-8925-a29c48be341a","currentValue":"","widgetInfo":{"widgetType":"text","name":"File","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"FwkParameters":{"nuid":"7e41eb47-0f03-439f-934f-c997ab92c1d6","currentValue":"","widgetInfo":{"widgetType":"text","name":"FwkParameters","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"SinkPath":{"nuid":"de26edac-02ab-4f7a-8fd4-5773018536d1","currentValue":"","widgetInfo":{"widgetType":"text","name":"SinkPath","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"SinkGlobalParameters":{"nuid":"9d10496f-1ac3-4d59-bee7-f1849a355a6f","currentValue":" { \"kv_scope_name\": \"mdmf-scope-dev\", \"kv_workspace_id\": \"mdmf-workspace-id\", \"kv_workspace_pk\": \"mdmf-workspace-pk\", \"ing_sink_storage_name\": \"mdmfadlsdev\", \"ing_sink_storage_secret_name\": \"MDMF-SinkStorageAccount-Key\", \"ing_sink_storage_type\": \"ADLS\", \"ing_sink_container_name\": \"sink\", \"dv_sink_storage_name\": \"mdmfadlsdev\", \"dv_sink_storage_secret_name\": \"MDMF-SinkStorageAccount-Key\", \"dv_sink_container_name\": \"sink\", \"dv_schema_container_name\": \"schemas\", \"dt_sink_storage_name\": \"mdmfadlsdev\", \"dt_sink_storage_secret_name\": \"MDMF-SinkStorageAccount-Key\", \"dt_sink_container_name\": \"datatransformation\" }","widgetInfo":{"widgetType":"text","name":"SinkGlobalParameters","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"ConvertedPath":{"nuid":"063fca5c-d8c4-40d0-b1bf-73dc10ae27d5","currentValue":"","widgetInfo":{"widgetType":"text","name":"ConvertedPath","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":2032585640865206}},"nbformat":4,"nbformat_minor":0}
