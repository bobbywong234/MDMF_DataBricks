{"cells":[{"cell_type":"markdown","source":["#### Data Transformation: Create Delta Lake Tables\nConnect to sink instance and create Delta Lake tables with the specified name and path"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Metadata-Driven Ingestion Framework","showTitle":true,"inputWidgets":{},"nuid":"ab9d0346-0eb9-470f-81b1-a5f80a950197"}}},{"cell_type":"code","source":["%run \"/Shared/MDMF/Tools/utilities\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3197f7be-be52-498d-a25d-f95f0da8aebf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import json\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col\nfrom delta.tables import *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Dependencies","showTitle":true,"inputWidgets":{},"nuid":"490f363a-40b4-48b9-a58c-b1ef497d930d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.widgets.text(\"DataTransformationParameters\", \"\", \"\")\nWidget_DataTransformationParameters = dbutils.widgets.get(\"DataTransformationParameters\")\n\ndbutils.widgets.text(\"SinkGlobalParameters\", \"\", \"\")\nWidget_SinkGlobalParameters = dbutils.widgets.get(\"SinkGlobalParameters\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Widgets","showTitle":true,"inputWidgets":{},"nuid":"9c5c16e1-2be1-4541-b18b-abddc5290a8a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["DataTransformationParameters = json.loads(Widget_DataTransformationParameters)\n\n\"\"\"WriteMode\"\"\"\nwrite_mode = DataTransformationParameters[\"WriteMode\"]\n\n\"\"\"InputParameters\"\"\"\ninput_parameters = json.loads(DataTransformationParameters[\"InputParameters\"])\n\n\"\"\"SourceDataSets\"\"\"\nsource_datasets_father = json.loads(DataTransformationParameters[\"SourceDatasets\"])\n#json.loads take a string as input and returns a dictionary as output.\n#json.dumps take a dictionary as input and returns a string as output.\nsource_datasets_son = json.loads(json.dumps(source_datasets_father[\"sourceDatasets\"]))\ndataset_a = json.loads(json.dumps(source_datasets_son[\"dataset1\"]))\nsource_module = dataset_a[\"sourceModule\"]\nsource_path = dataset_a[\"sourcePath\"]\nsource_file_format = dataset_a[\"sourceFileFormat\"]\n\n\"\"\"Output\"\"\"\noutput_path = DataTransformationParameters[\"OutputPath\"]\n\n\"\"\"Errors\"\"\"\nerrors = []\nnotebook_output = {}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"DataTransformationParameters to Vars","showTitle":true,"inputWidgets":{},"nuid":"f645b859-63c6-4733-81dc-84d6029dcfb9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["SinkGlobalParameters = json.loads(Widget_SinkGlobalParameters)\n\nscope_name = SinkGlobalParameters['kv_scope_name']\nkv_workspace_id = SinkGlobalParameters['kv_workspace_id']\nkv_workspace_pk = SinkGlobalParameters['kv_workspace_pk']\ning_sink_storage_type = SinkGlobalParameters['ing_sink_storage_type']\ndv_schema_container_name = SinkGlobalParameters['dv_schema_container_name']\n\n\"\"\"dynamic values\"\"\"\nstorage_account_name = ''\nstorage_access_key_secret_name = ''\ncontainer_name = ''\n\nif 'ingestion' in source_module.lower():\n  storage_account_name = SinkGlobalParameters['ing_sink_storage_name']\n  storage_access_key_secret_name = SinkGlobalParameters['ing_sink_storage_secret_name']\n  container_name = SinkGlobalParameters['ing_sink_container_name']\n\nif 'validation' in source_module.lower():\n  storage_account_name = SinkGlobalParameters['dv_sink_storage_name']\n  storage_access_key_secret_name = SinkGlobalParameters['dv_sink_storage_secret_name']\n  container_name = SinkGlobalParameters['dv_sink_container_name']\n\nif 'transformation' in source_module.lower():\n  storage_account_name = SinkGlobalParameters['dt_sink_storage_name']\n  storage_access_key_secret_name = SinkGlobalParameters['dt_sink_storage_secret_name']\n  container_name = SinkGlobalParameters['dt_sink_container_name']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"SinkGlobalParameters to Vars","showTitle":true,"inputWidgets":{},"nuid":"6195d727-f228-414f-89d8-26d8eea0c171"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def columnUpper(df, function_name, input_parameters, DataTransformationParameters):  \n  columns_dict = json.loads(json.dumps(input_parameters[\"functions\"][function_name]))\n  columns_list = columns_dict[\"columns\"]\n  print('Executing ColumnUpper Method...' + str(columns_list))\n  \n  for col in df.columns:\n    if len(columns_list) > 0:\n      for col_user in columns_list:\n        df = df.withColumn(col_user, F.upper(F.col(col_user))) # Upper specific columns\n    else:\n      df = df.withColumn(col, F.upper(F.col(col))) # Upper all columns  \n  return df\n\n\ndef columnLower(df, function_name, input_parameters, DataTransformationParameters):\n  columns_dict = json.loads(json.dumps(input_parameters[\"functions\"][function_name]))\n  columns_list = columns_dict[\"columns\"]\n  print('Executing ColumnLower Method...' + str(columns_list))\n\n  for col in df.columns:\n    if len(columns_list) > 0:\n      for col_user in columns_list:\n        df = df.withColumn(col_user, F.lower(F.col(col_user))) # Lower specific columns\n    else:\n      df = df.withColumn(col, F.lower(F.col(col))) # Lower all columns  \n  return df\n  \n  \ndef columnTrim(df, function_name, input_parameters, DataTransformationParameters):\n  columns_dict = json.loads(json.dumps(input_parameters[\"functions\"][function_name]))\n  columns_list = columns_dict[\"columns\"]\n  print('Executing ColumnTrim Method...' + str(columns_list))\n  \n  for col in df.columns:\n    if len(columns_list) > 0:\n      for col_user in columns_list:\n        df = df.withColumn(col_user, F.trim(F.col(col_user))) # Trim  specific columns\n    else:\n      df = df.withColumn(col, F.trim(F.col(col))) # Trim all columns\n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Functions: Upper Lower Trim","showTitle":true,"inputWidgets":{},"nuid":"8e9bd34b-d707-47e3-9be9-eba7b8b7f1d1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def validate_merge(SinkGlobalParameters, output_path):\n  try:\n    \"\"\"global parameters\"\"\"\n    scope_name = SinkGlobalParameters[\"kv_scope_name\"]    \n    storage_account_name = SinkGlobalParameters['dt_sink_storage_name']\n    storage_access_key_secret_name = SinkGlobalParameters['dt_sink_storage_secret_name']   \n    container_name = SinkGlobalParameters['dt_sink_container_name']\n    \n    spark.conf.set(\n    \"fs.azure.account.key.{}.dfs.core.windows.net\".format(storage_account_name), \n    dbutils.secrets.get(scope=scope_name, key=storage_access_key_secret_name))    \n        \n    delta_path = 'abfss://{}@{}.dfs.core.windows.net/{}'.format(container_name, storage_account_name, output_path)    \n    df_delta = spark.read.format(\"delta\").option(\"header\",\"true\").load(delta_path)\n    \n    if df_delta.rdd.isEmpty() == False:\n      print('Is a valid delta for -Merge-.')\n      return 'Merge'\n    else:\n      print('Merge is not possible (Invalid delta sink), changing mode from -Merge- to -Overwrite-.')\n      return 'Overwrite'\n      \n  except Exception as ex:\n    print('Merge is not possible (Invalid delta sink), changing mode from -Merge- to -Overwrite-.')\n    return 'Overwrite'    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Validate first merge","showTitle":true,"inputWidgets":{},"nuid":"b526596b-6589-4297-9b36-37ee1d2937ae"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def read_source_data(scope_name, storage_access_key_secret_name, storage_account_name, container_name, source_path, source_file_format):\n  try:\n    spark.conf.set(\n    \"fs.azure.account.key.{}.dfs.core.windows.net\".format(storage_account_name), \n    dbutils.secrets.get(scope=scope_name, key=storage_access_key_secret_name))\n    \n    full_source_path = \"abfss://{container}@{storage}.dfs.core.windows.net/{file}\".format(container=container_name, storage=storage_account_name, file=source_path)\n    \n    if source_file_format.lower() == 'parquet':\n      return spark.read.parquet(full_source_path)\n    if source_file_format.lower() == 'delta':\n      return spark.read.format(\"delta\").option(\"header\",\"true\").load(full_source_path)  \n  except Exception as ex:\n    errors.append('Error in read_source_data method: {}'.format(ex))\n    print(ex)\n\ndef save_delta_format(df, SinkGlobalParameters, output_path, write_mode, input_parameters):\n  try:\n    \"\"\"Global parameters\"\"\"\n    scope_name = SinkGlobalParameters[\"kv_scope_name\"]  \n    storage_access_key_secret_name = SinkGlobalParameters['dt_sink_storage_secret_name']   \n    container_name = SinkGlobalParameters['dt_sink_container_name']\n    storage_account_name = SinkGlobalParameters['dt_sink_storage_name']\n    \n    \"\"\"Generate connection\"\"\"\n    spark.conf.set(\n    \"fs.azure.account.key.{}.dfs.core.windows.net\".format(storage_account_name), \n    dbutils.secrets.get(scope=scope_name, key=storage_access_key_secret_name))\n    \n    \"\"\"Create final path to save delta\"\"\"\n    delta_path = 'abfss://{}@{}.dfs.core.windows.net/{}'.format(container_name, storage_account_name, output_path)   \n    \n    \"\"\"input parameters\"\"\"\n    delta_key_columns = input_parameters[\"deltaKeyColumns\"] # merge\n    delta_partition_columns = input_parameters[\"deltaPartitionColumns\"] # partition\n    delta_output_columns = input_parameters[\"deltaOutputColumns\"] # output        \n    \n    \"\"\"DeltaOutputColumns: save the result filtering by columns\"\"\"\n    if len(delta_output_columns) > 0:\n      df = df.select(delta_output_columns)\n      \n    \"\"\"Validate first merge and change to overwrite\"\"\"\n    if write_mode.lower() == 'merge':\n      write_mode = validate_merge(SinkGlobalParameters, output_path)   \n      \n    \"\"\"Merge: save the result merging df updates with delta base.\"\"\"\n    if write_mode.lower() == 'merge':\n      print('Saving delta {}...'.format(write_mode))\n      delta_table = DeltaTable.forPath(spark, delta_path)\n      key_list = []\n      \n      for key in delta_key_columns:\n        key_list.append('delta_table_current.{id} = df_transformed.{id}'.format(id=key))\n      \n      delta_table.alias('delta_table_current') \\\n        .merge(\n          df.alias('df_transformed'), \n          ' AND '.join(key_list)\n      ) \\\n      .whenMatchedUpdateAll() \\\n      .whenNotMatchedInsertAll() \\\n      .execute()    \n      print('Success: {} in {}'.format(write_mode, delta_path))\n    else:\n      \"\"\"DeltaPartitionColumns: save the result with partition using overwrite or append.\"\"\"\n      if len(delta_partition_columns) > 0:\n        print('Saving delta {} with partition {}...'.format(write_mode, delta_partition_columns))\n        df.write.format('delta') \\\n            .option(\"overwriteSchema\",True) \\\n            .partitionBy(delta_partition_columns) \\\n            .mode(write_mode.lower()) \\\n            .save(delta_path)\n        print('Success: {} with partition {} in {}'.format(write_mode, delta_partition_columns, delta_path))\n      else:\n        \"\"\"No Partition: save the result using overwrite or append.\"\"\"\n        print('Saving delta {} with no partition...'.format(write_mode))\n        df.write.format('delta') \\\n            .option(\"overwriteSchema\",True) \\\n            .mode(write_mode.lower()) \\\n            .save(delta_path)\n        print('Success: {} with no partition in {}'.format(write_mode, delta_path))\n  except Exception as ex:\n    errors.append('Error in save_delta_format method: {}'.format(ex))\n    print(ex)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read and save files","showTitle":true,"inputWidgets":{},"nuid":"2ba20f7c-99de-4d86-9aef-0ac730da3df1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\"\"\"read source data\"\"\"\ntry:\n  df = read_source_data(scope_name, storage_access_key_secret_name, storage_account_name, container_name, source_path, source_file_format)\nexcept Exception as ex:\n  print(ex)\n  errors.append('Error in read source data: {}'.format(ex))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Get source data","showTitle":true,"inputWidgets":{},"nuid":"e639282a-aab6-4c00-8ec8-f05b1390ddb3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["try:\n  \"\"\"\"execute functions dynamically\"\"\"\n  for function_name in input_parameters[\"functions\"]:  \n    df = locals()[function_name](df, function_name, input_parameters, DataTransformationParameters)\n  #save_delta_format(df, SinkGlobalParameters, output_path, write_mode, input_parameters)\n  finalschema=save_to_delta_format(df,'datatransformation',storage_account_name, output_path, write_mode, input_parameters, dbname= 'dataransformation',DeltaTableName='SimpleTransformations')\n\nexcept Exception as ex:\n    errors.append('Error in main for statement: {}'.format(ex))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Main","showTitle":true,"inputWidgets":{},"nuid":"753f153e-6007-4cd9-a7ea-49a83deddc73"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["if len(errors) > 0:\n  raise Exception(str(errors))\nnotebook_output = {'Errors':'. \\n'.join(errors)}\ndbutils.notebook.exit(finalschema)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Output","showTitle":true,"inputWidgets":{},"nuid":"0d3e0f47-0a37-4060-ab69-fc481383b0c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"NB_01_SimpleTransformations","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{"SinkGlobalParameters":{"nuid":"8da05484-f5b8-4256-84f9-8f27af7e48f0","currentValue":"{\"kv_scope_name\":\"MDMF-Scope-Test2\",\"kv_workspace_id\":\"mdmf-workspace-id\",\"kv_workspace_pk\":\"mdmf-workspace-pk\",\"schema_container_name\":\"schemas\",\"ing_sink_storage_name\":\"mdmfadlstest\",\"ing_sink_storage_secret_name\":\"MDMF-SinkStorageAccount-Key\",\"ing_sink_storage_type\":\"ADLS\",\"ing_sink_container_name\":\"sink\",\"dv_sink_storage_name\":\"mdmfadlstest\",\"dv_sink_storage_secret_name\":\"MDMF-SinkStorageAccount-Key\",\"dv_sink_container_name\":\"sink\",\"dv_schema_container_name\":\"schemas\",\"dt_sink_storage_name\":\"mdmfadlstest\",\"dt_sink_storage_secret_name\":\"MDMF-SinkStorageAccount-Key\",\"dt_sink_container_name\":\"datatransformation\"}","widgetInfo":{"widgetType":"text","name":"SinkGlobalParameters","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"DataTransformationParameters":{"nuid":"e20e8c61-083d-446d-9a67-470737588ed0","currentValue":"{    \"DtLogId\":5,    \"EntRunId\":\"c812525f-69a7-4356-9c06-d66b10028fb4\",    \"DtRunId\":\"21fde3b9-2d65-4499-a212-e2be5e82739d\",    \"PipelineTriggerDt\":\"2022-04-19T00:32:42.03\",    \"DtStatus\":null,    \"DtOutputId\":5,    \"DtConfigId\":1,    \"FunctionName\":\"SimpleTransformations\",    \"NotebookPath\":\"/Shared/Metadata Driven Ingestion Framework/DataTransformation/NB_01_SimpleTransformations\",    \"DtMethod\":\"Databricks\",    \"OutputPath\":\"AlfaRomeo\",    \"InputParameters\":\"{\\\"deltaKeyColumns\\\":[\\\"symboling\\\",\\\"normalizedlosses\\\",\\\"make\\\",\\\"fueltype\\\",\\\"aspiration\\\",\\\"wheelbase\\\",\\\"length\\\",\\\"price\\\"],\\\"deltaPartitionColumns\\\":[],\\\"deltaOutputColumns\\\":[],\\\"functions\\\":{\\\"columnUpper\\\":{\\\"columns\\\":[]}, \\\"columnTrim\\\":{\\\"columns\\\":[]}}}\",    \"WriteMode\":\"Merge\",    \"WriteModeId\":1,    \"SourceDatasets\":\"{ \\\"sourceDatasets\\\": { \\\"dataset1\\\": { \\\"sourceModule\\\": \\\"Data Ingestion\\\", \\\"sourcePath\\\": \\\"Converted/ADLS/CsvFolder/AlfaRomeo/2022/03/31/04/\\\", \\\"sourceFileFormat\\\": \\\"Parquet\\\" }} }\" }","widgetInfo":{"widgetType":"text","name":"DataTransformationParameters","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":46311312444454}},"nbformat":4,"nbformat_minor":0}
