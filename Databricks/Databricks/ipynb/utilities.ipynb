{"cells":[{"cell_type":"code","source":["import json\nimport requests\nimport datetime\nimport hashlib\nimport hmac\nimport base64\nimport numpy\nimport dateutil\nimport pandas as pd\nimport re\nfrom pyspark.sql import SparkSession\nimport functools\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n#from pyspark.sql import SparkSession\nimport time\n#import pprint\nimport numpy as np\nfrom delta.tables import *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Imports","showTitle":true,"inputWidgets":{},"nuid":"a9e8ae21-16af-4c50-b022-905f0b113768"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def mount_to_dbfs(kv_scope_name, storage_name, key_secret_name, container_name, folder_path):\n  mount_point = '/mnt/mdmf/temp/{}'.format(folder_path)  \n  source_path = \"wasbs://{container}@{storage}.blob.core.windows.net/{folder}\".format(container=container_name, storage=storage_name, folder=folder_path)\n  configs = {\"fs.azure.account.key.{}.blob.core.windows.net\".format(storage_name):dbutils.secrets.get(scope = kv_scope_name, key = key_secret_name)}\n  #source_path = \"abfss://{container}@{storage}.dfs.core.windows.net/{folder}\".format(container=container_name, storage=storage_name, folder=folder_path)\n  #configs = {\"fs.azure.account.key.{}.dfs.core.windows.net\".format(storage_name):dbutils.secrets.get(scope = kv_scope_name, key = key_secret_name)}\n  \n  if not any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n    try:\n      print('Mounting {}'.format(mount_point))\n      dbutils.fs.mount(\n        source = source_path,\n        mount_point = mount_point,\n        extra_configs = configs)\n      print('Mounted: {} -> {}'.format(source_path, mount_point))\n    except Exception as e:\n      print(\"Error ocurred when trying to mount: {}\".format(e))\n\n  return mount_point"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Mount","showTitle":true,"inputWidgets":{},"nuid":"abc09dde-5278-4989-9e10-1e9197e2ce23"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def unmount_to_dbfs(mount_point):\n  if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n    try:\n      print('Unmounting {}...'.format(mount_point))\n      dbutils.fs.unmount(mount_point)\n    except Exception as e:\n      print(\"Error ocurred when trying to unmount: {}\".format(e))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Unmount","showTitle":true,"inputWidgets":{},"nuid":"805e1ac6-5f18-4aa4-b231-e55cceb52e24"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# set column schema to pyspark datatype\ndef format_column_schema(json_schema):\n  schema = []\n  for item in json_schema:\n    column_name = item['name']\n    column_type = item['type'].lower()\n    \n    if \"varchar\" in column_type:\n      new_schema = f\"{column_name} STRING\"\n    elif column_type == 'int':\n      new_schema = f\"{column_name} INTEGER\"\n    elif column_type == 'bigint':\n      new_schema = f\"{column_name} LONG\"\n    elif \"decimal\" in column_type:\n      new_schema = f\"{column_name} {column_type.upper()}\"\n    elif column_type == 'datetime':  \n      new_schema = f\"{column_name} TIMESTAMP\"\n    elif column_type == 'bit':\n      new_schema = f\"{column_name} BOOLEAN\"\n    else:\n      new_schema = f\"{column_name} {column_type}\"\n    schema.append(new_schema)\n  \n  schema = ', '.join(schema)\n  \n  return schema\n\n#from shcema struct to sql \ndef schema2sql(structschema):\n  outschema=[]\n  for n, row in enumerate((structschema.jsonValue())['fields']):\n              data={}\n              data['id'] = str(n+1)\n              data['name'] = row['name']\n              types=['string','decimal','double','integer','boolean','timestamp','date','binary']\n              sqlt=['Varchar (255)', 'Decimal(18,4)','Decimal(18,4)','Int','Datetime','Date','Binary']\n              for i in range(len(types)):\n                if types[i] in row['type'].lower():\n                  data['type']=sqlt[i]\n                  break\n                else:\n                  data['type']=row['type']\n\n              outschema.append(data)\n  return outschema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Format sql schema into pyspark schema","showTitle":true,"inputWidgets":{},"nuid":"310d7b10-7472-4c74-acd7-adfe596d5437"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_global_values(module):\n  try:\n    if module.lower() == 'ingestion' or 'ingestion' in module.lower():\n      storage_access_key_secret_name = SinkGlobalParameters['ing_sink_storage_secret_name']\n      storage_account_name = SinkGlobalParameters['ing_sink_storage_name']      \n      container_name = SinkGlobalParameters['ing_sink_container_name']\n      return storage_access_key_secret_name, storage_account_name, container_name\n\n    if module.lower() == 'validation' or 'validation' in module.lower():\n      storage_access_key_secret_name = SinkGlobalParameters['dv_sink_storage_secret_name']\n      storage_account_name = SinkGlobalParameters['dv_sink_storage_name']      \n      container_name = SinkGlobalParameters['dv_sink_container_name']\n      return storage_access_key_secret_name, storage_account_name, container_name\n\n    if module.lower() == 'transformation' or 'transformation' in module.lower():\n      storage_access_key_secret_name = SinkGlobalParameters['dt_sink_storage_secret_name']\n      storage_account_name = SinkGlobalParameters['dt_sink_storage_name']      \n      container_name = SinkGlobalParameters['dt_sink_container_name']\n      return storage_access_key_secret_name, storage_account_name, container_name\n  except Exception as ex:    \n    errors.append('Error in get_global_values method: {}'.format(ex))\n    print(ex)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"*Get sink values by source module (ingestion, validation, transformation)","showTitle":true,"inputWidgets":{},"nuid":"be8c2e88-a240-490e-9c44-ba7f69fdc684"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def validate_merge(SinkGlobalParameters, output_path):\n  try:\n    \"\"\"global parameters\"\"\"\n    scope_name = SinkGlobalParameters[\"kv_scope_name\"]    \n    storage_account_name = SinkGlobalParameters['dt_sink_storage_name']\n    storage_access_key_secret_name = SinkGlobalParameters['dt_sink_storage_secret_name']   \n    container_name = SinkGlobalParameters['dt_sink_container_name']\n    \n    spark.conf.set(\n    \"fs.azure.account.key.{}.dfs.core.windows.net\".format(storage_account_name), \n    dbutils.secrets.get(scope=scope_name, key=storage_access_key_secret_name))    \n        \n    delta_path = 'abfss://{}@{}.dfs.core.windows.net/{}'.format(container_name, storage_account_name, output_path)    \n    df_delta = spark.read.format(\"delta\").option(\"header\",\"true\").load(delta_path)\n    \n    if df_delta.rdd.isEmpty() == False:\n      print('Is a valid delta for -Merge-.')\n      return 'Merge'\n    else:\n      print('Merge is not possible (Invalid delta sink), changing mode from -Merge- to -Overwrite-.')\n      return 'Overwrite'\n      \n  except Exception as ex:\n    print('Merge is not possible (Invalid delta sink), changing mode from -Merge- to -Overwrite-.')\n    return 'Overwrite'    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Validate merge","showTitle":true,"inputWidgets":{},"nuid":"92b87d04-1f1b-463d-bf06-cb4e11370880"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_source_data(scope_name, storage_account_name, storage_access_key_secret_name, container_name, source_path, source_file_format,columnschema=None):  \n  try:\n    spark.conf.set(\n    \"fs.azure.account.key.{}.dfs.core.windows.net\".format(storage_account_name), \n    dbutils.secrets.get(scope=scope_name, key=storage_access_key_secret_name))\n    \n    #spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", dbutils.secrets.get(scope=scope_name,key=storage_access_key_secret_name))\n\n    \n    spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n    #spark.conf.set(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")\n    \n    source_final_path = \"abfss://{container}@{storage}.dfs.core.windows.net/{file}\".format(container=container_name, storage=storage_account_name, file=source_path)\n    \n    if source_file_format.lower() == 'delta':\n      #df = spark.read.schema(columnschema).format(\"delta\").load(source_final_path)\n      df = spark.read.format(\"delta\").load(source_final_path)\n    else:\n      try:\n        df=spark.read.schema(columnschema).parquet(source_final_path)\n      except:\n        df = spark.read.parquet(source_final_path)  \n      \n    return df \n  except Exception as ex:\n    print(ex)\n    errors.append('Error in read_source_data method: {}'.format(ex))\n\ndef save_to_delta_format(df, container_name,storage_account_name, output_path, write_mode, input_parameters,vacuum=168,dbname='datavalidation',DeltaTableName='DeltaTable'):\n  try:\n    \"\"\"input parameters\"\"\"\n    delta_key_columns = input_parameters[\"deltaKeyColumns\"] # merge\n    delta_partition_columns = input_parameters[\"deltaPartitionColumns\"] # partition\n    delta_output_columns = input_parameters[\"deltaOutputColumns\"] # output\n    \n    table_path = 'abfss://{}@{}.dfs.core.windows.net/{}'.format(container_name, storage_account_name, output_path)\n    print(table_path)\n    #delta table config\n    spark.sql(\"SET spark.databricks.delta.formatCheck.enabled=false\")\n    spark.sql(\"CREATE DATABASE IF NOT EXISTS \" + dbname)\n   \n    \"\"\"DeltaOutputColumns: save the result filtering by columns\"\"\"\n    if len(delta_output_columns) > 0:\n      df = df.select(delta_output_columns)\n  \n    \"\"\"DeltaPartitionColumns: save the result by columns partitioning using append, overwrite or merge\"\"\"\n    if write_mode.lower() == 'merge':\n      print('Saving delta {}...'.format(write_mode))\n      delta_table = DeltaTable.forPath(spark, table_path) \n      # delete the ones with more than n hours\n      \n      \n      key_list = []\n      for key in delta_key_columns:\n        key_list.append('delta_table_current.{id} = df_transformed.{id}'.format(id=key))\n      \n      delta_table.alias('delta_table_current') \\\n        .merge(\n          df.alias('df_transformed'), \n          ' AND '.join(key_list)\n      ) \\\n      .whenMatchedUpdateAll() \\\n      .whenNotMatchedInsertAll() \\\n      .execute()      \n      print('Done: {} '.format(write_mode))\n    else:\n      if len(delta_partition_columns) > 0:\n        print('Saving delta {} with partition {}...'.format(write_mode, delta_partition_columns))\n        spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n        df.write.format('delta') \\\n            .option(\"overwriteSchema\",True) \\\n            .partitionBy(delta_partition_columns) \\\n            .mode(write_mode.lower()) \\\n            .save(table_path)\n        \n        \n        print('Done: {} with partition {}.'.format(write_mode, delta_partition_columns))\n      else:\n        print('Saving delta {} with no partition...'.format(write_mode))\n        \n        df.write.format('delta') \\\n              .option(\"overwriteSchema\",True) \\\n              .mode(write_mode.lower()) \\\n              .save(table_path)\n      \n      \n      #create DELTA TABLE\n        \n    df.write.format('delta') \\\n      .option(\"overwriteSchema\",True) \\\n      .partitionBy(delta_partition_columns) \\\n      .mode(write_mode.lower()) \\\n      .saveAsTable(dbname+'.'+DeltaTableName)\n          \n    print('Done: {} with no partition.'.format(write_mode))\n    #schema for the validated df\n    dfschema=schema2sql(df.schema)\n    #vacuum the ones that have more than 168 hours old\n    delta_table = DeltaTable.forPath(spark, table_path)\n    delta_table.vacuum(vacuum)\n    \n    return dfschema\n\n  except Exception as ex:\n    print(ex)\n    errors.append('Error in save_delta_format method: {}'.format(ex))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"READ PARQUET DELTA AND SAVE TO DELTA","showTitle":true,"inputWidgets":{},"nuid":"78affbbc-259f-4ffa-b70e-179ab5c4d401"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def build_signature(customer_id, shared_key, date, content_length, method, content_type, resource):\n  x_headers = 'x-ms-date:' + date\n  string_to_hash = method + \"\\n\" + str(content_length) + \"\\n\" + content_type + \"\\n\" + x_headers + \"\\n\" + resource\n  authorization = \"\"\n  try:\n    bytes_to_hash = str.encode(string_to_hash,'utf-8')  \n    decoded_key = base64.b64decode(shared_key)\n    encoded_hash = (base64.b64encode(hmac.new(decoded_key, bytes_to_hash, digestmod=hashlib.sha256).digest())).decode()\n    authorization = \"SharedKey {}:{}\".format(customer_id,encoded_hash)\n  except Exception as i:\n    print('Error in Utilities/build_signature: ',i)\n    raise Exception(f'Error: {i}')\n  return authorization"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Build API signature","showTitle":true,"inputWidgets":{},"nuid":"a3f52492-aec7-494b-959f-c94775c2153c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def post_data(scope, customer_id, shared_key, output):\n  log_type = 'MDMFDatabricksLog'\n  method = 'POST'\n  content_type = 'application/json'\n  resource = '/api/logs'\n  body = json.dumps(output)\n  content_length = len(body)\n  #Retrieve your Log Analytics Workspace ID from your Key Vault Databricks Secret Scope\n  wks_id = dbutils.secrets.get(scope = scope, key = customer_id)\n  #Retrieve your Log Analytics Primary Key from your Key Vault Databricks Secret Scope\n  wks_shared_key = dbutils.secrets.get(scope = scope, key = shared_key)  \n  rfc1123date = datetime.datetime.utcnow().strftime('%a, %d %b %Y %H:%M:%S GMT')\n  signature = build_signature(wks_id, wks_shared_key, rfc1123date, content_length, method, content_type, resource)\n  uri = 'https://' + wks_id + '.ods.opinsights.azure.com' + resource + '?api-version=2016-04-01'\n\n  headers = {\n      'content-type': content_type,\n      'Authorization': signature,\n      'Log-Type': log_type,\n      'x-ms-date': rfc1123date\n  }\n\n  response = requests.post(uri,data=body, headers=headers)\n  if (response.status_code >= 200 and response.status_code <= 299):\n      print ('Log Accepted')\n  else:\n      print (\"Log Response code: {}\".format(response.status_code))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Send POST request to api/logs","showTitle":true,"inputWidgets":{},"nuid":"e23343e1-3ee2-450b-ab95-ea3f66c611cb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def return_storage_name(instance_url):\n  return (instance_url.split(\".\")[0]).split(\"/\")[-1]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Return the Storage Name from instance URL","showTitle":true,"inputWidgets":{},"nuid":"fa1c1632-470c-4bcb-8b9d-84a018b90d41"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def return_file_path(container_name, storage_name, file_path):\n  return \"abfss://{}@{}.dfs.core.windows.net/{}\".format(container_name, storage_name, file_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Returns ADLS full file path url","showTitle":true,"inputWidgets":{},"nuid":"684f46f6-bc19-47c7-bff6-a15a8a5291db"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_from_adls(kv_scope_name, service_principal_id, service_principal_secret_name, tenant_id, storage_name, container_name, file_path, file_extension, object_schema=None, column_schema=None):  \n  service_credential = dbutils.secrets.get(scope=kv_scope_name,key=service_principal_secret_name)\n\n  spark.conf.set(f\"fs.azure.account.auth.type.{storage_name}.dfs.core.windows.net\", \"OAuth\")\n  spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n  spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_name}.dfs.core.windows.net\", service_principal_id)\n  spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_name}.dfs.core.windows.net\", service_credential)\n  spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n\n  spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n  \n  full_file_path = return_file_path(container_name, storage_name, file_path)\n  \n  folder_path = file_path[0:file_path.rfind(\"/\")]\n  file_name = file_path.split(\"/\")[-1]\n  file_extension = file_extension.lower()\n  file_delimiter = \"|\" #pipe as default\n  header_flag = \"true\" #header as default\n  #getting values from object_schema\n  if object_schema:\n    if ('delimiter' in object_schema):\n      file_delimiter = object_schema['delimiter']\n    if ('headerFlag' in object_schema):\n      if object_schema['headerFlag'].upper() == 'Y' or object_schema['headerFlag'].upper() == 'TRUE':\n        header_flag = \"true\"\n      else:\n        header_flag = \"false\"\n\n  if file_extension  == 'csv':\n    if column_schema:\n      ##read with schema\n      df = spark.read.schema(column_schema).format(\"csv\").option(\"header\",header_flag).load(full_file_path)\n    else:\n      ##read with no schema\n      df = spark.read.format(\"csv\").option(\"header\",header_flag).load(full_file_path)\n  elif file_extension  == 'txt':\n    if column_schema:\n      ##read with schema\n      df = spark.read.schema(column_schema).option(\"header\", header_flag).option(\"delimiter\",file_delimiter).csv(full_file_path)\n    else:\n      ##read with no schema\n      df = spark.read.option(\"header\", header_flag).option(\"delimiter\",file_delimiter).csv(full_file_path)\n  elif file_extension == 'json':\n    if column_schema:\n      ##read with schema\n      df = spark.read.schema(column_schema).option(\"multiline\",\"true\").json(full_file_path)\n    else:\n      ##read with no schema\n      df = spark.read.option(\"multiline\",\"true\").json(full_file_path)\n  elif file_extension == 'parquet':\n    if column_schema:\n      ##read with schema\n      df = spark.read.schema(column_schema).parquet(full_file_path)\n    else:\n      ##read with no schema\n      df = spark.read.parquet(full_file_path)\n  elif file_extension == 'delta':\n    spark.sql(\"SET spark.databricks.delta.formatCheck.enabled=false\")\n    if column_schema:\n      ##read with schema\n      df = spark.read.schema(column_schema).format(\"delta\").load(full_file_path)\n    else:\n      ##read with no schema\n      df = spark.read.format(\"delta\").load(full_file_path)\n  elif file_extension == 'xml':\n    mount_path = mount_to_dbfs(kv_scope_name, storage_name, key_secret_name, container_name, folder_path)\n    mount_full_path = '/dbfs' + mount_path + '/' + file_name\n\n    df_pd = pd.read_xml(mount_full_path)\n    df_pd = df_pd.replace('nan','NaN').fillna('NaN')\n\n    if column_schema:\n      ##read with schema\n      df = spark.createDataFrame(df_pd, column_schema)\n    else:\n      ##read with no schema\n      df = spark.createDataFrame(df_pd)\n  elif file_extension  == 'xls' or file_extension=='xlsx':\n    #import glob\n    #path = '/dbfs/mnt/mdmf/temp/FS/VM1_FS/Financial/Full/Financial_20220715215952'\n    #path2 = '/dbfs/mnt/mdmf/temp/FS/VM1_FS/Financial/Full/Financial_20220715215952/*.csv'\n    #for filename in glob.glob(path):\n    #    print(filename)\n    #print(path)\n\n    !pip install openpyxl\n    !pip install xlrd\n\n    mount_path = mount_to_dbfs(kv_scope_name, storage_name, key_secret_name, container_name, folder_path)\n    mount_full_path = '/dbfs' + mount_path + '/' + file_name\n\n    #sc = SparkContext.getOrCreate()\n    #spark_session = SparkSession(sc)\n    df_pd = pd.read_excel(mount_full_path, engine='openpyxl')\n    df_pd = df_pd.replace('nan','NaN').fillna('NaN').replace('?','NaN')\n    df_pd2 = df_pd.rename(columns=lambda x: x.replace(\" \", '_'))\n    df_pd2 = df_pd2.rename(columns=lambda x: x.replace(\":\" , ''))\n\n    if column_schema:\n      ##read with schema\n      df = spark.createDataFrame(df_pd2, column_schema)\n    else:\n      ##read with no schema\n      df = spark.createDataFrame(df_pd2)\n  else:\n    raise Exception('The function \"read_from_adls\" does not support the file extension: {}'.format(file_extension))\n\n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read from ADLS","showTitle":true,"inputWidgets":{},"nuid":"64ed9f80-4e17-4326-80b4-ac9762a35892"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_to_adls(kv_scope_name, service_principal_id, service_principal_secret_name, tenant_id, storage_name, container_name, file_path, file_extension, df, object_schema=None, write_mode=None, object_name=None):  \n  \n  service_credential = dbutils.secrets.get(scope=kv_scope_name,key=service_principal_secret_name)\n\n  spark.conf.set(f\"fs.azure.account.auth.type.{storage_name}.dfs.core.windows.net\", \"OAuth\")\n  spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n  spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_name}.dfs.core.windows.net\", service_principal_id)\n  spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_name}.dfs.core.windows.net\", service_credential)\n  spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n\n  spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n  \n  full_file_path = return_file_path(container_name, storage_name, file_path)\n  \n  if not write_mode:\n      write_mode = \"overwrite\"\n  write_mode = write_mode.lower()\n  file_extension = file_extension.lower()\n  file_delimiter = \"|\" #pipe as default\n  header_flag = \"true\" #header as default\n  primary_key = \"\"\n  #getting values from object_schema\n  if object_schema:\n    if ('delimiter' in object_schema):\n      file_delimiter = object_schema['delimiter']\n    if ('headerFlag' in object_schema):\n      if object_schema['headerFlag'].upper() == 'Y' or object_schema['headerFlag'].upper() == 'TRUE':\n        header_flag = \"true\"\n      else:\n        header_flag = \"false\"\n    if ('primaryKey' in object_schema):\n      primary_key = object_schema['primaryKey']\n  \n  if file_extension == 'parquet': \n    df.write.format(\"parquet\").mode(write_mode).save(full_file_path)\n  elif file_extension == 'delta':\n    \n    #If full_file_path does not exists yet then is assumed to be the first execution therefore write_mode is changed to \"overwrite\"\n    try:\n      dbutils.fs.ls(full_file_path)\n    except Exception as ex:\n      if 'FileNotFoundException' in str(ex):\n        print('Delta path does not exist yet, write_method changed to overwrite')\n        write_mode = \"overwrite\"\n      else:\n        raise Exception('ERROR: {}'.format(ex))\n    \n    if write_mode == 'merge':\n      if not primary_key:\n        raise Exception('PrimaryKey is not defined, please add the primary key column list on the ObjectSchema column in FwkObjectMetadata table.')\n      \n      delta_table = DeltaTable.forPath(spark, full_file_path)\n      df.createOrReplaceTempView(\"df_update\")\n      \n      key_list = []\n      for key in primary_key.split(','):\n        key_list.append('delta_table.{id} = df_update.{id}'.format(id=key.strip()))\n      key_list = ' AND '.join(key_list)\n      \n      spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\") #Allow schema evolution\n      \n      delta_table.alias('delta_table') \\\n       .merge(\n          df.alias('df_update'), \n          key_list) \\\n       .whenMatchedUpdateAll() \\\n       .whenNotMatchedInsertAll() \\\n       .execute()\n    else:\n      df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(write_mode).save(full_file_path) #Allow schema evolution\n  elif file_extension  == 'csv':\n    df.write.format(\"csv\").option(\"header\", header_flag).mode(write_mode).save(full_file_path)\n  elif file_extension  == 'txt':\n    df.write.format(\"csv\").option(\"header\", header_flag).options(\"delimiter\",file_delimiter).mode(write_mode).save(full_file_path)\n  elif file_extension == 'json':\n    df.coalesce(1).write.format('json').save(full_file_path)\n  elif file_extension == 'xml':\n    df.write.format(\"com.databricks.spark.xml\")\\\n    .option(\"rootTag\", object_name + 's')\\\n    .option(\"rowTag\", object_name)\\\n    .mode(write_mode)\\\n    .save(full_file_path)\n  elif file_extension  == 'xls' or file_extension=='xlsx':\n    df.write.format(\"csv\").option(\"header\", header_flag).mode(write_mode).save(full_file_path)\n  else:\n    raise Exception('The function \"write_to_adls\" does not support the file extension: {}'.format(file_extension))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Write to ADLS","showTitle":true,"inputWidgets":{},"nuid":"b887f53a-4c6f-456d-8800-09be7869dba3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"utilities","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":559456846063552}},"nbformat":4,"nbformat_minor":0}
