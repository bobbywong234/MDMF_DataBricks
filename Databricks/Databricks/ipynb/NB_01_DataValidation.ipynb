{"cells":[{"cell_type":"code","source":["%run \"/Shared/MDMF/Tools/utilities\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"642c1c1a-9a7a-49e4-abd9-aa1a83de5815"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import json\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col\nfrom delta.tables import *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Dependencies","showTitle":true,"inputWidgets":{},"nuid":"490f363a-40b4-48b9-a58c-b1ef497d930d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.widgets.text(\"DataValidationParameters\", \"\", \"\")\nWidget_DataValidationParameters = dbutils.widgets.get(\"DataValidationParameters\")\n\ndbutils.widgets.text(\"SinkGlobalParameters\", \"\", \"\")\nWidget_SinkGlobalParameters = dbutils.widgets.get(\"SinkGlobalParameters\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Widgets","showTitle":true,"inputWidgets":{},"nuid":"9c5c16e1-2be1-4541-b18b-abddc5290a8a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["DataValidationParameters = json.loads(Widget_DataValidationParameters)\n\ndvlog_id = DataValidationParameters[\"DvLogId\"]\n\"\"\"WriteMode\"\"\"\nwrite_mode = DataValidationParameters[\"WriteMode\"]\n\n\"\"\"InputParameters\"\"\"\ninput_parameters = json.loads(DataValidationParameters[\"InputParameters\"])\n\n\n\"\"\"SourceDataSets\"\"\"\nimport re\nDataValidationParameters[\"SourceDatasets\"]=re.sub('[\\n\\t\\r]','',DataValidationParameters[\"SourceDatasets\"])\n\n\nsource_datasets_father = json.loads(DataValidationParameters[\"SourceDatasets\"])\n\n#json.loads take a string as input and returns a dictionary as output.\n#json.dumps take a dictionary as input and returns a string as output.\nsource_datasets_son = json.loads((DataValidationParameters[\"SourceDatasets\"]).replace('\\r\\n\\t\\t\\t\\t\\t',''))['sourceDatasets']\ndsval=[x for x in source_datasets_son.keys()][0]\ndataset_a = json.loads(json.dumps(source_datasets_son[dsval]))\nsource_module = dataset_a[\"sourceModule\"]\nsource_path = dataset_a[\"sourcePath\"]\nlanding_path=dataset_a[\"landingPath\"]\nsource_file_format = dataset_a[\"sourceFileFormat\"]\ncolumnSchema=json.loads(dataset_a['columnSchema'])\ncolumnschema = [i ['name'] for i in columnSchema] #list of columns only\n\n\"\"\"Output\"\"\"\noutput_path = DataValidationParameters[\"OutputPath\"]\n\n\"\"\"Errors\"\"\"\nerrors = []\nnotebook_output = {}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"DataTransformationParameters to Vars","showTitle":true,"inputWidgets":{},"nuid":"f645b859-63c6-4733-81dc-84d6029dcfb9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">JSONDecodeError</span>                           Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4051856122285748&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>DataValidationParameters <span class=\"ansi-blue-fg\">=</span> json<span class=\"ansi-blue-fg\">.</span>loads<span class=\"ansi-blue-fg\">(</span>Widget_DataValidationParameters<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> dvlog_id <span class=\"ansi-blue-fg\">=</span> DataValidationParameters<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;DvLogId&#34;</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> <span class=\"ansi-blue-fg\">&#34;&#34;&#34;WriteMode&#34;&#34;&#34;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> write_mode <span class=\"ansi-blue-fg\">=</span> DataValidationParameters<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;WriteMode&#34;</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-green-fg\">/usr/lib/python3.8/json/__init__.py</span> in <span class=\"ansi-cyan-fg\">loads</span><span class=\"ansi-blue-fg\">(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    355</span>             parse_int <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">and</span> parse_float <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">and</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    356</span>             parse_constant is None and object_pairs_hook is None and not kw):\n<span class=\"ansi-green-fg\">--&gt; 357</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> _default_decoder<span class=\"ansi-blue-fg\">.</span>decode<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    358</span>     <span class=\"ansi-green-fg\">if</span> cls <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    359</span>         cls <span class=\"ansi-blue-fg\">=</span> JSONDecoder\n\n<span class=\"ansi-green-fg\">/usr/lib/python3.8/json/decoder.py</span> in <span class=\"ansi-cyan-fg\">decode</span><span class=\"ansi-blue-fg\">(self, s, _w)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    335</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    336</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 337</span><span class=\"ansi-red-fg\">         </span>obj<span class=\"ansi-blue-fg\">,</span> end <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>raw_decode<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">,</span> idx<span class=\"ansi-blue-fg\">=</span>_w<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>end<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    338</span>         end <span class=\"ansi-blue-fg\">=</span> _w<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">,</span> end<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>end<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    339</span>         <span class=\"ansi-green-fg\">if</span> end <span class=\"ansi-blue-fg\">!=</span> len<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/lib/python3.8/json/decoder.py</span> in <span class=\"ansi-cyan-fg\">raw_decode</span><span class=\"ansi-blue-fg\">(self, s, idx)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    351</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    352</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 353</span><span class=\"ansi-red-fg\">             </span>obj<span class=\"ansi-blue-fg\">,</span> end <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>scan_once<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">,</span> idx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    354</span>         <span class=\"ansi-green-fg\">except</span> StopIteration <span class=\"ansi-green-fg\">as</span> err<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    355</span>             <span class=\"ansi-green-fg\">raise</span> JSONDecodeError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Expecting value&#34;</span><span class=\"ansi-blue-fg\">,</span> s<span class=\"ansi-blue-fg\">,</span> err<span class=\"ansi-blue-fg\">.</span>value<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n\n<span class=\"ansi-red-fg\">JSONDecodeError</span>: Unterminated string starting at: line 1 column 1010 (char 1009)</div>","errorSummary":"<span class=\"ansi-red-fg\">JSONDecodeError</span>: Unterminated string starting at: line 1 column 1010 (char 1009)","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">JSONDecodeError</span>                           Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4051856122285748&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>DataValidationParameters <span class=\"ansi-blue-fg\">=</span> json<span class=\"ansi-blue-fg\">.</span>loads<span class=\"ansi-blue-fg\">(</span>Widget_DataValidationParameters<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> dvlog_id <span class=\"ansi-blue-fg\">=</span> DataValidationParameters<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;DvLogId&#34;</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> <span class=\"ansi-blue-fg\">&#34;&#34;&#34;WriteMode&#34;&#34;&#34;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> write_mode <span class=\"ansi-blue-fg\">=</span> DataValidationParameters<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;WriteMode&#34;</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-green-fg\">/usr/lib/python3.8/json/__init__.py</span> in <span class=\"ansi-cyan-fg\">loads</span><span class=\"ansi-blue-fg\">(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    355</span>             parse_int <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">and</span> parse_float <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">and</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    356</span>             parse_constant is None and object_pairs_hook is None and not kw):\n<span class=\"ansi-green-fg\">--&gt; 357</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> _default_decoder<span class=\"ansi-blue-fg\">.</span>decode<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    358</span>     <span class=\"ansi-green-fg\">if</span> cls <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    359</span>         cls <span class=\"ansi-blue-fg\">=</span> JSONDecoder\n\n<span class=\"ansi-green-fg\">/usr/lib/python3.8/json/decoder.py</span> in <span class=\"ansi-cyan-fg\">decode</span><span class=\"ansi-blue-fg\">(self, s, _w)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    335</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    336</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 337</span><span class=\"ansi-red-fg\">         </span>obj<span class=\"ansi-blue-fg\">,</span> end <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>raw_decode<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">,</span> idx<span class=\"ansi-blue-fg\">=</span>_w<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>end<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    338</span>         end <span class=\"ansi-blue-fg\">=</span> _w<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">,</span> end<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>end<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    339</span>         <span class=\"ansi-green-fg\">if</span> end <span class=\"ansi-blue-fg\">!=</span> len<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/lib/python3.8/json/decoder.py</span> in <span class=\"ansi-cyan-fg\">raw_decode</span><span class=\"ansi-blue-fg\">(self, s, idx)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    351</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    352</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 353</span><span class=\"ansi-red-fg\">             </span>obj<span class=\"ansi-blue-fg\">,</span> end <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>scan_once<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">,</span> idx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    354</span>         <span class=\"ansi-green-fg\">except</span> StopIteration <span class=\"ansi-green-fg\">as</span> err<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    355</span>             <span class=\"ansi-green-fg\">raise</span> JSONDecodeError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Expecting value&#34;</span><span class=\"ansi-blue-fg\">,</span> s<span class=\"ansi-blue-fg\">,</span> err<span class=\"ansi-blue-fg\">.</span>value<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n\n<span class=\"ansi-red-fg\">JSONDecodeError</span>: Unterminated string starting at: line 1 column 1010 (char 1009)</div>"]}}],"execution_count":0},{"cell_type":"code","source":["SinkGlobalParameters = json.loads(Widget_SinkGlobalParameters)\n\nkv_scope_name = SinkGlobalParameters['kv_scope_name']\nkv_workspace_id = SinkGlobalParameters['kv_workspace_id']\nkv_workspace_pk = SinkGlobalParameters['kv_workspace_pk']\ning_sink_storage_type = SinkGlobalParameters['ing_sink_storage_type']\ndv_schema_container_name = SinkGlobalParameters['dv_schema_container_name']\n\n\n\"\"\"dynamic values\"\"\"\nstorage_account_name = ''\nstorage_access_key_secret_name = ''\ncontainer_name = ''\n\nif 'ingestion' in source_module.lower():\n  storage_account_name = SinkGlobalParameters['ing_sink_storage_name']\n  storage_access_key_secret_name = SinkGlobalParameters['ing_sink_storage_secret_name']\n  container_name = SinkGlobalParameters['ing_sink_container_name']\n\nif 'validation' in source_module.lower():\n  storage_account_name = SinkGlobalParameters['dv_sink_storage_name']\n  storage_access_key_secret_name = SinkGlobalParameters['dv_sink_storage_secret_name']\n  container_name = SinkGlobalParameters['dv_sink_container_name']\n\nadls_source_name = '{}.dfs.core.windows.net/'.format(storage_account_name)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"SinkGlobalParameters to Vars","showTitle":true,"inputWidgets":{},"nuid":"6195d727-f228-414f-89d8-26d8eea0c171"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["DataValidationParameters"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba15e776-2d0b-4808-a4c2-e6729b726a93"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["class DataValidation: # All the validations available pending great expect py library \n  \n  def __init__(self,dataframe=None):\n    print('Data Validation MDMF')\n    \n    if dataframe==None or dataframe.count()<1:\n\n        #self.dataframe = read_pqt(kv_scope_name, storage_access_key_secret_name,\n         #              storage_account_name,container_name, source_path)\n        self.dataframe=read_source_data(kv_scope_name, storage_account_name,storage_access_key_secret_name,\n                       container_name, source_path,source_file_format)\n        \n    else:\n      self.dataframe=dataframe\n\n\n#VALIDATION FOR ROW COUNT\n  def rowcount(self,*args,**kwargs):\n    self.function_name = kwargs.get('function_name',\"default value\")\n    self.Dvmethod = kwargs.get('Dvmethod',\"default value\")\n    self.input_dict = kwargs.get('input_dict',\"default value\")\n    try:\n      \"\"\"Read data Output Files and create delta tables \"\"\"\n      spark.conf.set(\"fs.azure.account.key.{}.dfs.core.windows.net\".format(storage_account_name),\"{}\".format(dbutils.secrets.get(scope = \"{}\".format(kv_scope_name), key= \"{}\".format(storage_access_key_secret_name))))    \n      source_path2=source_path.replace(source_path.split('/')[-1],'')\n      df_converted = read_source_data(kv_scope_name, storage_account_name,storage_access_key_secret_name,\n                       container_name, source_path,source_file_format)\n      #read_pqt(kv_scope_name, storage_access_key_secret_name, storage_account_name, container_name, source_path2,self.colschema)\n      row_count_converted = df_converted.count() \n\n      row_count_landing = 0\n\n      if source_file_format.lower() == 'parquet' or source_file_format.lower() == 'delta':\n        df_landing = read_source_data(kv_scope_name, storage_account_name,storage_access_key_secret_name,\n                       container_name, source_path,source_file_format) \n        #read_pqt(kv_scope_name, storage_access_key_secret_name, storage_account_name, container_name, landing_path )\n        row_count_landing = df_landing.count()\n      else:\n        extension_file = source_file_format.lower()\n        path_landing = \"abfss://{}@{}{}\".format(container_name, adls_source_name, landing_path)\n\n        \"\"\" xml, xls and xlsx need be mounted using /mnt/... \"\"\"\n        if extension_file == 'xml': # XML \n          file_name = landing_path.split('/')[-1]\n          \n          landing_path2 = landing_path.replace(file_name, '')\n          mnt_path = mount_to_mnt(landing_path2, kv_scope_name, storage_access_key_secret_name,storage_account_name,container_name)\n          df = pd.read_xml(mnt_path + file_name)        \n          row_count_landing = len(df)      \n        elif extension_file  == 'xls' or extension_file=='xlsx':\n          file_name = landing_path.split('/')[-1]\n          landing_path2 = landing_path.replace(file_name, '')\n          mnt_path = mount_to_mnt(landing_path2, kv_scope_name, storage_access_key_secret_name,storage_account_name,container_name)\n          df=pd.read_excel(mnt_path+file_name) \n          row_count_landing = len(df)\n\n          \"\"\" Read landing path directly without utilities: \"\"\"\n        elif extension_file == 'csv':          \n          df_landing = spark.read.format(\"csv\").option(\"header\",\"true\").load(path_landing)\n          row_count_landing = df_landing.count()          \n        elif extension_file == 'parquet':\n          df_landing = spark.read.parquet(path_landing)\n          row_count_landing = df_landing.count()\n        elif extension_file == 'parquet folder':\n          df_landing = spark.read.parquet(path_landing + '*.parquet')\n          print('parquet folder, path_landing: {}'.format(path_landing))\n          row_count_landing = df_landing.count()          \n        elif extension_file == 'json':          \n          df_landing = spark.read.option(\"multiline\",\"true\").json(path_landing)\n          row_count_landing = df_landing.count()\n        elif extension_file  == 'txt':\n          headerList=spark.read.csv(path_landing).take(1) \n          delims=['|',';',',','\\t','\\n','\\\\','/','//']\n          delimiter=\" \"\n          for d in delims:\n            if headerList[0][0].find(d)!=-1:\n                delimiter=d\n            else:\n              pass                \n          df_landing = spark.read.option(\"header\", \"true\").option(\"delimiter\",delimiter).csv(path_landing)   #as of spark1.6 you can use  csv to read txt\n          row_count_landing = df_landing.count()\n      \"\"\" Validation \"\"\"\n      if row_count_converted == row_count_landing:\n        validation_status = \"Succeeded\"\n        message = \"RowCout Validation was applied. Source and Converted records match\"\n      else:\n        validation_status = \"Failed\"\n        message = \"Source and sink records do not match. Source count: {} , Sink count {}.\".format(str(row_count_landing),str(row_count_converted))\n\n      json_output = {\"ExecutionStatus\": \"Successfull\",\"DvLogId\": DataValidationParameters['DvLogId'], \"Output\": {\"Count\": str(row_count_converted), \"Validation\": { \"Status\": validation_status, \"Message\": message}}}      \n    except Exception as ex:    \n      raise Exception(f'Error: {ex}')\n      msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail to execute function to validate source type and count records','DvLogId': dvlog_id,'FunctionName': self.function_name ,'DvMethod':self.Dvmethod}\n      #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\n    return (json_output,self.dataframe)\n    \n  def nullcount(self,*args,**kwargs): #split between valid and invalid data of nulls\n    self.function_name = kwargs.get('function_name',\"default value\")\n    self.Dvmethod = kwargs.get('Dvmethod',\"default value\")\n    self.input_dict = kwargs.get('input_dict',\"default value\")\n\n    def union_dataframes(dfs):\n      \"\"\"Will union two or more dataframes into one single dataframe.\"\"\"\n      return functools.reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)\n    \n    def get_corrupted_data(dataframe, column_names, get_df=True):\n      \"\"\"Will get the Valid and Invalid data just using the index.\"\"\"\n      with_null_values = []\n      message_result = []\n      total_array = []\n      empty = sqlContext.createDataFrame(sc.emptyRDD(), dataframe.schema)\n      for item in column_names:\n        sin_data = dataframe.filter(col(item).isNull() | isnan(col(item)))\n        message_result.append(f\"Column '{item}' with '{sin_data.count()}' null values.\")\n        total_array.append(str(sin_data.count()))\n\n        if sin_data.count() != 0 or sin_data.rdd.isEmpty():\n          with_null_values.append(sin_data)\n\n      if get_df:\n        unioned_df = union_dataframes(with_null_values)\n        unioned_df = unioned_df.dropDuplicates()\n        nullCount_result = '\\n'.join(message_result)\n\n      else:\n        unioned_df = []\n        nullCount_result = '\\n'.join(message_result)\n\n      result_data = {\n                      \"unioned_df\": unioned_df,\n                      \"nullCount_result\": nullCount_result,\n                      \"total_array\": total_array\n                     }\n\n      return result_data\n\n\n    def condition_output(dataframe, column_names, dv_sink_container_name,\n                         sink_path, dv_sink_storage_name, kv_scope_name, kv_workspace_id,\n                         kv_workspace_pk, function_name, dv_method, dvlog_id):\n\n      \"\"\"Will split up a Dataframe into two new dataframes, one for Valid Data and one for Invalid Data, this depends on 'get-df' value if True.\"\"\"\n      try:\n        #corrupt_data, nullCount_result, total_array\n\n        result_data = get_corrupted_data(self.dataframe, column_names)\n\n        if result_data['unioned_df'].count() != 0:\n\n          SinkValid = sink_path + '/_NullCount_Valid'\n          SinkInvalid = sink_path + '/_NullCount_Invalid'\n          \n          # joining valid and invalidad data\n          dataframes_join = dataframe.union(result_data['unioned_df'])\n          DF_join = dataframes_join.toPandas()\n\n          # Invalid data\n          DF_No_corrupted = DF_join.drop_duplicates(keep=False)\n\n          # Valid data\n          DF_corrupted = result_data['unioned_df'].toPandas()\n\n          print(f\"* Corrupted records: {len(DF_corrupted)}\")\n          print(f\"* No corrupted records: {len(DF_No_corrupted)}\")\n\n          if len(DF_No_corrupted) > 0:\n          # writing valid data\n            correctSpk = spark.createDataFrame(DF_No_corrupted)\n            self.dataframe=correctSpk #df wil now be input for future validations\n            save_to_delta_format(correctSpk,'datavalidation',storage_account_name, output_path+SinkValid, write_mode, input_parameters,DeltaTableName='nullcount') # save to delta\n\n          if len(DF_corrupted) > 0:\n          # writing invalid data\n            corrupt = DF_No_corrupted.fillna(np.nan)\n            corruptSpk = spark.createDataFrame(corrupt)\n            save_to_delta_format(corruptSpk,'datavalidation',storage_account_name, output_path+SinkInvalid, write_mode, input_parameters,DeltaTableName='nullcountinvalid')\n          print(\"\\n> Currated files saved successfully.\")\n          \n        return result_data['nullCount_result'], result_data['total_array']\n      except Exception as ex:\n        print(f\"*** ERROR in condition_output: {ex}\")\n        msg_error = {'ExecutionStatus': 'Failed', 'Error Message': 'Fail to execute function split data', 'DvLogId': dvlog_id, 'FunctionName': self.function_name , 'DvMethod': self.Dvmethod}\n        #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\n\n    try:\n      \n      message_result, total_array = condition_output(self.dataframe, self.input_dict['columns'], container_name,\n                                                   output_path, storage_account_name, kv_scope_name, kv_workspace_id,\n                                                   kv_workspace_pk, self.function_name, self.Dvmethod, dvlog_id)\n      print(message_result, total_array)\n      validation_status = \"Succeeded\"\n      json_output={'ExecutionStatus': 'Successfull', 'DvLogId': dvlog_id, 'Output': {'Count': total_array, 'Validation': { 'Status': validation_status, 'Message': message_result}}}\n    except Exception as err:\n      print(f\"An error has occurred...{err}\")\n      msg_error = {'ExecutionStatus': 'Failed', 'Error Message': 'Fail to Build the inputs for the DataValidationLog table', 'DvLogId': dvlog_id, 'FunctionName': self.function_name , 'DvMethod': self.Dvmethod}\n     # post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\n      json_output = msg_error\n    return (json_output,self.dataframe)\n\n      \n\n#### COLUMN LEVEL VALIDATION###\n  def columnlevel(self,*args,**kwargs): #split valid and invlid regex\n    self.function_name = kwargs.get('function_name',\"default value\")\n    self.Dvmethod = kwargs.get('Dvmethod',\"default value\")\n    self.input_dict = kwargs.get('input_dict',\"default value\")\n    input_dict=self.input_dict['columns'][0]\n\n  \n    def get_expressions(input_parameter_dict):\n      \"\"\"Get the regular expresion as well as column's name\"\"\"\n      try:\n        columns = []\n        expressions = []\n        key_pairs = str(input_parameter_dict).split(',')\n\n        for item in key_pairs:\n\n          key_pair = item.split(':')\n          column = key_pair[0].replace(\"{\",\"\").replace('\"','').replace(\"'\",'').strip()\n          key_value = key_pair[1].replace(\"}\",\"\").replace('\"','').replace(\"'\",'').strip()\n\n          columns.append(column)\n          expressions.append(key_value)\n\n        return columns, expressions\n\n      except Exception as ex:\n\n        print(f\"**** ERROR {ex}\")\n        #return get_list_regex(input_parameter_dict)\n\n    # Function to count values that do not meet the regex parameter in a specified coulmn from the loaded file\n    def count_regex(dataframe, column_names, target_expressions):                                 \n      \"\"\"Count the number of input parameter in file.\"\"\"\n\n      result = {}\n      indexes = []\n\n      try:\n        pd_dataframe = dataframe.toPandas()\n        for column, reg_exp in zip(column_names, target_expressions):\n          print(f\"* Column: {column} | Regex: {reg_exp}\")\n\n          null_values = []\n          no_match = []\n          match = []\n\n          for index_b, item in enumerate(pd_dataframe[column]):\n\n            if item is None:\n              null_values.append(int(index_b))\n\n              if int(index_b) not in indexes:\n                indexes.append(int(index_b))\n\n            #regex function expects string so convert item to string\n            elif not re.match(reg_exp, str(item), flags=0):\n              no_match.append(int(index_b))\n\n              if int(index_b) not in indexes:\n                indexes.append(int(index_b))\n\n            else:\n              match.append(int(index_b))\n\n          result[column] = {\"null\": len(null_values), \"match\": len(match), \"no_match\": len(no_match)}\n\n        result['corrupted_indexes'] = indexes\n        result['records'] = len(pd_dataframe)\n\n\n      except Exception as ex:\n\n        print(f\"*** ERROR in count_regex: {ex}\")\n        msg_error = {'ExecutionStatus': 'Failed', 'Error Message': 'Fail to execute function to count values that do not meet the regex parameter in a specified column from the loaded file', 'DvLogId': dvlog_id, 'FunctionName': self.function_name , 'DvMethod': self.Dvmethod}\n        #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\n        result = msg_error\n      return (result)\n\n\n    def split_columnlevel(dataframe, sink_regex_array, sink_container_name, sink_path, adls_storage_account_name):  \n      try:\n        pd_dataframe = dataframe.toPandas()\n\n        if sink_regex_array['corrupted_indexes']:\n\n          SinkValid = sink_path + '/_ColumnLevel_Valid'\n          SinkInvalid = sink_path + '/_ColumnLevel_Invalid'\n\n          corrupted = pd_dataframe.loc[sink_regex_array['corrupted_indexes']]\n          no_corrupted = pd_dataframe.drop(sink_regex_array['corrupted_indexes'])\n          \n\n          # No corrupted data\n          if len(no_corrupted) > 0:\n            no_corrupted_Spk = spark.createDataFrame(no_corrupted)\n            self.dataframe=no_corrupted_Spk\n            save_to_delta_format(no_corrupted_Spk,'datavalidation',storage_account_name, output_path+SinkValid, write_mode, input_parameters,DeltaTableName='columnlevel')\n\n          # Corrupted data\n          corrupted = corrupted.fillna('')\n          corrupted_Spk = spark.createDataFrame(corrupted)\n          save_to_delta_format(corrupted_Spk,'datavalidation',storage_account_name, output_path+SinkInvalid, write_mode, input_parameters,DeltaTableName='columnlevelinvalid')\n          \n\n          print(\"> Currated (Valid & Invalid) files loaded\")\n          \n      except Exception as ex:\n        print(f\"*** ERROR in condition_output: {ex}\")\n        msg_error = {'ExecutionStatus': 'Failed', 'Error Message': 'Fail to execute function to split data', 'DvLogId': dvlog_id, 'FunctionName': self.function_name , 'DvMethod': self.Dvmethod}\n        #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\n        return msg_error\n    # Build the inputs for the DataValidationLog table\n    def Validate_Column_Regex(sink_regex_array):\n      \"\"\"An Output will generate to indicate if the process were well or an error was found.\"\"\"\n      \n      try:\n        message = ''\n\n        for key, value_e in sink_regex_array.items():\n\n          if isinstance(value_e, dict):\n            message += f\"The column '{key}' has '{value_e['null']} null', '{value_e['match']} do match', '{value_e['no_match']} no match' values.\\n\"\n\n        validation_status = \"Succeeded\"\n        output = {'ExecutionStatus': 'Successfull', 'DvLogId': dvlog_id, 'Output': {'Count': sink_regex_array['records'], 'Validation': {'Status': validation_status, 'Message': message}}}\n\n        return output\n\n      except Exception as ex:\n\n        print(f\"*** ERROR in Validate_Column_Regex: {ex}\")\n        msg_error = {'ExecutionStatus': 'Failed', 'Error Message': 'Fail to execute Build the inputs for the DataValidationLog table', 'DvLogId': dvlog_id, 'FunctionName': self.function_name , 'DvMethod': self.Dvmethod}\n       # post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\n        return msg_error  \n    try:\n\n\n      column_names, target_expressions = get_expressions(input_dict)  \n      sink_regex_array = count_regex(self.dataframe, column_names, target_expressions)\n      split_columnlevel(self.dataframe, sink_regex_array, container_name, output_path,storage_account_name)\n\n      json_output = Validate_Column_Regex(sink_regex_array)\n    except Exception as ex:\n      print(f\"*** ERROR: {ex}\")\n      msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail at Checking columns with regex','DvLogId': dvlog_id,'FunctionName': self.function_name ,'DvMethod':self.Dvmethod}\n     # post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\n      json_output = msg_error\n    return (json_output,self.dataframe)\n\n\n## RECORD LEVEL VALIDATION   \n  def recordlevel(self,*args,**kwargs): #split valid and invalid \n    self.function_name = kwargs.get('function_name',\"default value\")\n    self.Dvmethod = kwargs.get('Dvmethod',\"default value\")\n    self.input_dict = kwargs.get('input_dict',\"default value\")\n    target_count=self.input_dict['Count']\n\n  \n    def count_columns(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_container_name, source_path):\n      \"\"\"Will count the number of columns of a Dataframe.\"\"\"\n      try:\n        print(\"> Reading file\")\n        size = len(self.dataframe.columns)\n        return size\n      except Exception as ex:\n        print(f\"*** ERROR: {ex}\")\n        \n    try:\n        sink_column_count = count_columns(kv_scope_name, storage_access_key_secret_name, storage_account_name, container_name, source_path)\n        if target_count == sink_column_count:\n          validation_status = \"Succeeded\"\n          validation_bool = \"True\"\n          message = \"Target Count parameter and sink column count do match.\"\n\n        else:    \n          validation_status = \"Failed\"\n          validation_bool = \"False\"\n          message = \"Target Count parameter and sink column count do not match. Target count: {} but were found {}\".format(target_count, sink_column_count)\n        \n        json_output={'ExecutionStatus': 'Successfull', 'DvLogId': dvlog_id, 'Output': {'Count': sink_column_count, 'Validation': {'Status': validation_bool, 'Message': message}}}\n\n       # post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, json_output)\n    except Exception as ex:\n        print(f\"*** ERROR: {ex}\")\n        msg_error = {'ExecutionStatus': 'Failed', 'Error Message': 'Fail in Record Level', 'DvLogId': dvlog_id, 'FunctionName': self.function_name ,'DvMethod': self.Dvmethod}\n      #  post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\n        json_output = msg_error\n    return(json_output,self.dataframe)\n      \n\n## MIN MAX ROW COUNT VALIDATION\n  def minmaxrowcount(self,*args,**kwargs): #no split\n    print('MIN MAX ROW COUNT')\n    self.function_name = kwargs.get('function_name',\"default value\")\n    self.Dvmethod = kwargs.get('Dvmethod',\"default value\")\n    self.input_dict = kwargs.get('input_dict',\"default value\")\n    \n    min_value = self.input_dict['Min Value']\n    max_value = self.input_dict['Max Value']\n    \n    \n    def Compare_Count_Files(sink_file_rows_count, min_value, max_value, dvlog_id): #helper function for minmaxrow\n      \"\"\" Compares the files and validates its row counts are between the given range\"\"\"\n    # Build the inputs for the DataValidationLog table\n    # If Min Value is defined as zero, null, or empty -> Min Value will be taken as zero.\n      if min_value <= max_value:\n        if isinstance(min_value, str):\n          if min_value.lower() == \"null\" or min_value == \"\":\n            min_value = 0\n          else: \n            min_value = int(min_value)\n\n        if isinstance(max_value, str):\n          if max_value.lower() == \"null\" or max_value == \"\":\n            max_value = 0\n          else:\n            max_value = int(max_value)\n        # Validate that the sink record count is inside the thresholds.\n        if max_value == 0:\n          if sink_file_rows_count >= min_value:\n            validation_status = \"Succeeded\"\n            validation_bool = \"True\"\n            message = f\"MinMaxRowCount was applied. The values are between {min_value} and {max_value}\"\n          else:\n            validation_status = \"Failed\"\n            validation_bool = \"False\"\n            message = \"MinMaxRowCount was applied. The number of records is not higher than {}. The actual number is: {}.\".format(str(min_value), str(sink_file_rows_count))\n        else:\n          if sink_file_rows_count >= min_value and sink_file_rows_count <= max_value:\n            validation_status = \"Succeeded\"\n            validation_bool = \"True\"\n            message = \"MinMaxRowCount was applied.\"\n          else:\n            validation_status = \"Failed\"\n            validation_bool = \"False\"\n            message = \"MinMaxRowCount was applied. The number of records is not between {} and {}. The actual number is: {}.\".format(str(min_value), str(max_value), str(sink_file_rows_count))\n      else:\n        validation_status = \"Failed\"\n        validation_bool = \"False\"\n        message = \"MinMaxRowCount was applied. The min value is greater than the max value.\"\n\n      output = {'ExecutionStatus': 'Successfull', 'DvLogId': dvlog_id, 'Output': {'Count': sink_file_rows_count, 'Validation': { 'Status': validation_bool, 'Message': message}}}\n\n      return output\n    try:\n      df=read_source_data(kv_scope_name, storage_account_name,storage_access_key_secret_name,\n                       container_name, source_path,source_file_format)\n      json_output = Compare_Count_Files(df.count(), min_value, max_value, DataValidationParameters['DvLogId'])\n      #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, json_output)\n    except Exception as err:\n      raise Exception(f\"{err}\")\n      msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail at Comparing rows MinMaxRow counts','DvLogId': dvlog_id,'FunctionName': self.function_name ,'DvMethod':self.Dvmethod}\n    #  post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\n      json_output=msg_error\n    return (json_output,self.dataframe)\n\n  \n#### SCHEMA DRIFT VALIDATION\n  def schemadrift(self,*args,**kwargs): #split data between columns present and not present\n    self.function_name = kwargs.get('function_name',\"default value\")\n    self.Dvmethod = kwargs.get('Dvmethod',\"default value\")\n    self.input_dict = kwargs.get('input_dict',\"default value\")\n    try:\n      allow_evolution_bool = self.input_dict['Allow Evolution']\n    except:\n      allow_evolution_bool=False\n    \n    #Comparing schemas\n    schema_file_columns = columnschema\n    sink_file_columns = self.dataframe.columns\n    new_cols=list(set(sink_file_columns).difference(schema_file_columns))\n    del_cols=list(set(schema_file_columns).difference(sink_file_columns))\n    valid_cols=list(set(sink_file_columns).difference(new_cols+del_cols))\n    \n    # Build the inputs for the DvLog table\n    def Build_Logs(del_cols_list, new_cols_list,fwklog_id,allow_evolution_bool):\n      \"\"\"Builds the logs for the DVLog table.\"\"\"\n      validation_bool = \"Succeeded\"\n      if new_cols_list: \n        count_drifted_values = len(del_cols_list) + len(new_cols_list)\n      else:\n        count_drifted_values = 0\n      if del_cols_list != [] or new_cols_list != []:\n        message = \"Schema drift identified.\"\n        if del_cols_list != []:\n          message += f\"Deleted columns: {del_cols_list}.\\n\"\n          if allow_evolution_bool.lower() == \"false\":\n            validation_bool = \"Failed\"\n        if new_cols_list != []:\n          message += f\"Added columns: {new_cols_list}.\\n\"\n          if allow_evolution_bool.lower() == \"false\":\n            validation_bool = \"Failed\"\n      else:\n        message = \"\"\n      output = {'ExecutionStatus': 'Successfull', 'dvLogId': dvlog_id, 'Output': {'Drifted Values Count': str(count_drifted_values), 'Validation': {'Status': validation_bool, 'Message': message}}}\n      return output\n      \n    try:\n      #splitting the data\n      SinkValid = output_path + '/_Schema_Valid'      \n      correctSpk = self.dataframe.select(valid_cols)\n      self.dataframe=correctSpk\n\n      \n      SinkInvalid = output_path + '/_Schema_Invalid'\n      corruptSpk = self.dataframe.select(new_cols+del_cols)\n      if not allow_evolution_bool:\n        save_to_delta_format(correctSpk,'datavalidation',storage_account_name, output_path+SinkValid, write_mode, input_parameters,DeltaTableName='schema') # save to delta\n        if CorruptSpk.count()>0:\n          save_to_delta_format(corruptSpk,'datavalidation',storage_account_name, output_path+SinkInvalid, write_mode, input_parameters,DeltaTableName='schemainvalid')\n\n      \n      json_output = Build_Logs(del_cols, new_cols, dvlog_id, allow_evolution_bool)\n    except Exception as ex:\n      print(\"ERROR: {}\".format(ex))\n      msg_error = {'ExecutionStatus': 'Failed', 'Error Message': 'Fail at Getting schema and comparing with sink file', 'dvLogId': dvlog_id, 'FunctionName': self.function_name , 'DvMethod': self.Dvmethod}\n     # post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\n      json_output = msg_error\n    return json_output,self.dataframe\n  \n  #for complex validations like orphans\n  def orphans(self,*args,**kwargs):\n    \n    sqlQueryValid = kwargs.get('sqlQueryValid',\"default value\")\n    sqlQueryInvalid = kwargs.get('sqlQueryInvalid',\"default value\")\n    #Create temporary views to perform sql query from input parameters\n    for n,dataset_x in enumerate(source_datasets_son):\n      source_path = source_datasets_son[dataset_x]['sourcePath']\n      source_file_format = source_datasets_son[dataset_x]['sourceFileFormat']\n      columnschema = set_format_schema(json.loads(source_datasets_son[dataset_x]['columnSchema']))\n      storage_access_key_secret_name, storage_account_name, container_name = get_global_values(source_datasets_son[dataset_x][\"sourceModule\"])\n\n\n      df = read_source_data(kv_scope_name,storage_account_name, storage_access_key_secret_name,container_name, source_path, source_file_format)\n      df.createOrReplaceTempView(\"dataset{}\".format(n+1))\n      df.unpersist()\n      \n    try:\n      dfValid = spark.sql(sqlQueryValid)\n      save_to_delta_format(dfValid,'datavalidation',storage_account_name, output_path+'/Validated', write_mode, input_parameters,DeltaTableName='orphans')\n      self.dataframe=dfValid\n      dfInValid = spark.sql(sqlQueryInvalid)\n      \n      save_to_delta_format(dfInValid,'datavalidation',storage_account_name, output_path+'/Invalid', write_mode, input_parameters,DeltaTableName='oprhansinvalid')\n      json_output={'ExecutionStatus': 'Successfull', 'dvLogId': dvlog_id, 'Output': 'Orphans validation performed', 'Validation': {'Status': 'Succeeded', 'Message': f'found {dfValid.count()} records for the Valid query and {dfInValid.count()} for the Invalid '}}\n      \n    except Exception as ex:\n      json_output={'ExecutionStatus': 'Error', 'dvLogId': dvlog_id, 'Output': 'An error occurred in the validation', 'Validation': {'Status': 'Failed', 'Message': ex}}\n    for n2,dataset_x in enumerate(source_datasets_son):\n      spark.catalog.dropTempView(\"dataset{}\".format(n2+1))\n    print('orphans validation finished')\n    return json_output, self.dataframe\n      \n\n  def validate(self,functions):\n    outputfinal=[]\n\n    for function_name in functions:  \n      print(f'Performing {function_name} Validation')\n      if function_name.lower()=='orphans':\n        sqlQueryValid = functions[function_name]['sqlQueryValid']\n        sqlQueryInvalid = functions[function_name]['sqlQueryInvalid']\n        outputfinal.append(getattr(self, function_name.lower())(input_dict=functions[function_name],function_name=function_name,Dvmethod='Databricks',sqlQueryValid=sqlQueryValid,sqlQueryInvalid=sqlQueryInvalid)[0])\n      \n      else:\n        outputfinal.append(getattr(self, function_name.lower())(input_dict=functions[function_name],function_name=function_name,Dvmethod='Databricks')[0])\n\n        \n    #write Final dataframe to delta\n    self.dataframe = (getattr(self, function_name.lower())(input_dict=functions[function_name],function_name=function_name,Dvmethod='Databricks')[1])\n    finalschema=save_to_delta_format(self.dataframe,'datavalidation',storage_account_name, output_path+'/FinalValidation', write_mode, input_parameters,DeltaTableName=DataValidationParameters['FunctionName'])\n    \n    return outputfinal,self.dataframe,finalschema\n \n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1b6e44e-1159-40f7-b03a-e74a774067cd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["notebookoutput=DataValidation().validate(input_parameters['functions'])\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8302dbf5-d1ef-4639-bf48-3317166c00e4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["notebookoutput"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9663dafc-a9c4-49c5-aeec-8d57081890c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["if len(errors) > 0:\n  notebookoutput = {'Errors':'. \\n'.join(errors)}\n  raise Exception(str(errors))\ndbutils.notebook.exit(notebookoutput[2])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Output","showTitle":true,"inputWidgets":{},"nuid":"0d3e0f47-0a37-4060-ab69-fc481383b0c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"NB_01_DataValidation","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{"SinkGlobalParameters":{"nuid":"8da05484-f5b8-4256-84f9-8f27af7e48f0","currentValue":"","widgetInfo":{"widgetType":"text","name":"SinkGlobalParameters","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"DataTransformationParameters":{"nuid":"e20e8c61-083d-446d-9a67-470737588ed0","currentValue":"","widgetInfo":{"widgetType":"text","name":"DataTransformationParameters","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"DataValidationParameters":{"nuid":"49a0c702-0b82-4cc8-a28d-22bcbe228a4b","currentValue":"","widgetInfo":{"widgetType":"text","name":"DataValidationParameters","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":2688752529978741}},"nbformat":4,"nbformat_minor":0}
