{"cells":[{"cell_type":"markdown","source":["#### Data Transformation: Create Delta Lake Tables\nConnect to sink instance and create Delta Lake tables with the specified name and path"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Metadata-Driven Ingestion Framework","showTitle":true,"inputWidgets":{},"nuid":"a813e59c-f901-47a9-bd79-48b3d796ba69"}}},{"cell_type":"code","source":["%run \"/Shared/MDMF/Tools/utilities\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da1f304c-0d47-4216-8434-21fd01abf1fb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import json\nfrom pyspark.sql import functions as F\nfrom delta.tables import *\nfrom pyspark.sql.types import StructType"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Dependencies","showTitle":true,"inputWidgets":{},"nuid":"490f363a-40b4-48b9-a58c-b1ef497d930d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.widgets.text(\"DataTransformationParameters\", \"\", \"\")\nWidget_DataTransformationParameters = dbutils.widgets.get(\"DataTransformationParameters\")\n\ndbutils.widgets.text(\"SinkGlobalParameters\", \"\", \"\")\nWidget_SinkGlobalParameters = dbutils.widgets.get(\"SinkGlobalParameters\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Widgets","showTitle":true,"inputWidgets":{},"nuid":"9c5c16e1-2be1-4541-b18b-abddc5290a8a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["DataTransformationParameters = json.loads(Widget_DataTransformationParameters)\n\n\"\"\"WriteMode\"\"\"\nwrite_mode = DataTransformationParameters[\"WriteMode\"]\n\n\"\"\"InputParameters\"\"\"\ninput_parameters = json.loads(DataTransformationParameters[\"InputParameters\"])\n\n\"\"\"SourceDataSets\"\"\"\nsource_datasets_father = json.loads(DataTransformationParameters[\"SourceDatasets\"])\n#json.loads take a string as input and returns a dictionary as output.\n#json.dumps take a dictionary as input and returns a string as output.\nsource_datasets_son = json.loads(json.dumps(source_datasets_father[\"sourceDatasets\"]))\n\n\"\"\"Output\"\"\"\noutput_path = DataTransformationParameters[\"OutputPath\"]\n\n\"\"\"Errors\"\"\"\nerrors = []\nnotebook_output = {}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"DataTransformationParameters to Vars","showTitle":true,"inputWidgets":{},"nuid":"f645b859-63c6-4733-81dc-84d6029dcfb9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["SinkGlobalParameters = json.loads(Widget_SinkGlobalParameters)\n\nscope_name = SinkGlobalParameters['kv_scope_name']\nkv_workspace_id = SinkGlobalParameters['kv_workspace_id']\nkv_workspace_pk = SinkGlobalParameters['kv_workspace_pk']\ning_sink_storage_type = SinkGlobalParameters['ing_sink_storage_type']\ndv_schema_container_name = SinkGlobalParameters['dv_schema_container_name']\n\n\"\"\"dynamic values\"\"\"\nstorage_account_name = ''\nstorage_access_key_secret_name = ''\ncontainer_name = ''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"SinkGlobalParameters to Vars","showTitle":true,"inputWidgets":{},"nuid":"6195d727-f228-414f-89d8-26d8eea0c171"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def joinDatasets(df, function_name, input_parameters, DataTransformationParameters):\n  query = json.loads(json.dumps(input_parameters[\"functions\"][\"joinDatasets\"][\"Query\"]))\n  df = sqlContext.sql(query)\n  return df\n\n\ndef columnUpper(df, function_name, input_parameters, DataTransformationParameters):  \n  columns_dict = json.loads(json.dumps(input_parameters[\"functions\"][function_name]))\n  columns_list = columns_dict[\"columns\"]\n  print('Executing ColumnUpper Method...' + str(columns_list))\n  \n  for col in df.columns:\n    if len(columns_list) > 0:\n      for col_user in columns_list:\n        df = df.withColumn(col_user, F.upper(F.col(col_user))) # Upper specific columns\n    else:\n      df = df.withColumn(col, F.upper(F.col(col))) # Upper all columns  \n  return df\n\n\ndef columnLower(df, function_name, input_parameters, DataTransformationParameters):\n  columns_dict = json.loads(json.dumps(input_parameters[\"functions\"][function_name]))\n  columns_list = columns_dict[\"columns\"]\n  print('Executing ColumnLower Method...' + str(columns_list))\n\n  for col in df.columns:\n    if len(columns_list) > 0:\n      for col_user in columns_list:\n        df = df.withColumn(col_user, F.lower(F.col(col_user))) # Lower specific columns\n    else:\n      df = df.withColumn(col, F.lower(F.col(col))) # Lower all columns  \n  return df\n  \n  \ndef columnTrim(df, function_name, input_parameters, DataTransformationParameters):\n  columns_dict = json.loads(json.dumps(input_parameters[\"functions\"][function_name]))\n  columns_list = columns_dict[\"columns\"]\n  print('Executing ColumnTrim Method...' + str(columns_list))\n  \n  for col in df.columns:\n    if len(columns_list) > 0:\n      for col_user in columns_list:\n        df = df.withColumn(col_user, F.trim(F.col(col_user))) # Trim  specific columns\n    else:\n      df = df.withColumn(col, F.trim(F.col(col))) # Trim all columns\n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Functions: *Join, Upper, Lower, Trim","showTitle":true,"inputWidgets":{},"nuid":"8e9bd34b-d707-47e3-9be9-eba7b8b7f1d1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_global_values(module):\n  try:\n    if module.lower() == 'ingestion' or 'ingestion' in module.lower():\n      storage_access_key_secret_name = SinkGlobalParameters['ing_sink_storage_secret_name']\n      storage_account_name = SinkGlobalParameters['ing_sink_storage_name']      \n      container_name = SinkGlobalParameters['ing_sink_container_name']\n      return storage_access_key_secret_name, storage_account_name, container_name\n\n    if module.lower() == 'validation' or 'validation' in module.lower():\n      storage_access_key_secret_name = SinkGlobalParameters['dv_sink_storage_secret_name']\n      storage_account_name = SinkGlobalParameters['dv_sink_storage_name']      \n      container_name = SinkGlobalParameters['dv_sink_container_name']\n      return storage_access_key_secret_name, storage_account_name, container_name\n\n    if module.lower() == 'transformation' or 'transformation' in module.lower():\n      storage_access_key_secret_name = SinkGlobalParameters['dt_sink_storage_secret_name']\n      storage_account_name = SinkGlobalParameters['dt_sink_storage_name']      \n      container_name = SinkGlobalParameters['dt_sink_container_name']\n      return storage_access_key_secret_name, storage_account_name, container_name\n  except Exception as ex:    \n    errors.append('Error in get_global_values method: {}'.format(ex))\n    print(ex)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"*Get sink values by source module (ingestion, validation, transformation)","showTitle":true,"inputWidgets":{},"nuid":"7fab2e7d-1fcb-4fda-b071-a5dec96002e0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def validate_merge(SinkGlobalParameters, output_path):\n  try:\n    \"\"\"global parameters\"\"\"\n    scope_name = SinkGlobalParameters[\"kv_scope_name\"]    \n    storage_account_name = SinkGlobalParameters['dt_sink_storage_name']\n    storage_access_key_secret_name = SinkGlobalParameters['dt_sink_storage_secret_name']   \n    container_name = SinkGlobalParameters['dt_sink_container_name']\n    \n    spark.conf.set(\n    \"fs.azure.account.key.{}.dfs.core.windows.net\".format(storage_account_name), \n    dbutils.secrets.get(scope=scope_name, key=storage_access_key_secret_name))    \n        \n    delta_path = 'abfss://{}@{}.dfs.core.windows.net/{}'.format(container_name, storage_account_name, output_path)    \n    df_delta = spark.read.format(\"delta\").option(\"header\",\"true\").load(delta_path)\n    \n    if df_delta.rdd.isEmpty() == False:\n      print('Is a valid delta for -Merge-.')\n      return 'Merge'\n    else:\n      print('Merge is not possible (Invalid delta sink), changing mode from -Merge- to -Overwrite-.')\n      return 'Overwrite'\n      \n  except Exception as ex:\n    print('Merge is not possible (Invalid delta sink), changing mode from -Merge- to -Overwrite-.')\n    return 'Overwrite'    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Validate first merge","showTitle":true,"inputWidgets":{},"nuid":"f637f198-e2bf-48b0-a171-bf48bad434a8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def read_source_data(scope_name, storage_access_key_secret_name, storage_account_name, container_name, source_path, source_file_format):\n  try:\n    spark.conf.set(\n    \"fs.azure.account.key.{}.dfs.core.windows.net\".format(storage_account_name), \n    dbutils.secrets.get(scope=scope_name, key=storage_access_key_secret_name))\n    \n    full_source_path = \"abfss://{container}@{storage}.dfs.core.windows.net/{file}\".format(container=container_name, storage=storage_account_name, file=source_path)\n    \n    if source_file_format.lower() == 'parquet':\n      return spark.read.parquet(full_source_path)\n    if source_file_format.lower() == 'delta':\n      return spark.read.format(\"delta\").option(\"header\",\"true\").load(full_source_path)  \n  except Exception as ex:\n    errors.append('Error in read_source_data method: {}'.format(ex))\n    print(ex)\n\ndef save_delta_format(df, SinkGlobalParameters, output_path, write_mode, input_parameters):\n  try:\n    \"\"\"Global parameters\"\"\"\n    scope_name = SinkGlobalParameters[\"kv_scope_name\"]  \n    storage_access_key_secret_name = SinkGlobalParameters['dt_sink_storage_secret_name']   \n    container_name = SinkGlobalParameters['dt_sink_container_name']\n    storage_account_name = SinkGlobalParameters['dt_sink_storage_name']\n    \n    \"\"\"Generate connection\"\"\"\n    spark.conf.set(\n    \"fs.azure.account.key.{}.dfs.core.windows.net\".format(storage_account_name), \n    dbutils.secrets.get(scope=scope_name, key=storage_access_key_secret_name))\n    \n    \"\"\"Create final path to save delta\"\"\"\n    delta_path = 'abfss://{}@{}.dfs.core.windows.net/{}'.format(container_name, storage_account_name, output_path)   \n    \n    \"\"\"input parameters\"\"\"\n    delta_key_columns = input_parameters[\"deltaKeyColumns\"] # merge\n    delta_partition_columns = input_parameters[\"deltaPartitionColumns\"] # partition\n    delta_output_columns = input_parameters[\"deltaOutputColumns\"] # output        \n    \n    \"\"\"DeltaOutputColumns: save the result filtering by columns\"\"\"\n    if len(delta_output_columns) > 0:\n      df = df.select(delta_output_columns)\n      \n    \"\"\"Validate first merge and change to overwrite\"\"\"\n    if write_mode.lower() == 'merge':\n      write_mode = validate_merge(SinkGlobalParameters, output_path)   \n      \n    \"\"\"Merge: save the result merging df updates with delta base.\"\"\"\n    if write_mode.lower() == 'merge':\n      print('Saving delta {}...'.format(write_mode))\n      delta_table = DeltaTable.forPath(spark, delta_path)\n      key_list = []\n      \n      for key in delta_key_columns:\n        key_list.append('delta_table_current.{id} = df_transformed.{id}'.format(id=key))\n      \n      delta_table.alias('delta_table_current') \\\n        .merge(\n          df.alias('df_transformed'), \n          ' AND '.join(key_list)\n      ) \\\n      .whenMatchedUpdateAll() \\\n      .whenNotMatchedInsertAll() \\\n      .execute()    \n      print('Success: {} in {}'.format(write_mode, delta_path))\n    else:\n      \"\"\"DeltaPartitionColumns: save the result with partition using overwrite or append.\"\"\"\n      if len(delta_partition_columns) > 0:\n        print('Saving delta {} with partition {}...'.format(write_mode, delta_partition_columns))\n        df.write.format('delta') \\\n            .option(\"overwriteSchema\",True) \\\n            .partitionBy(delta_partition_columns) \\\n            .mode(write_mode.lower()) \\\n            .save(delta_path)\n        print('Success: {} with partition {} in {}'.format(write_mode, delta_partition_columns, delta_path))\n      else:\n        \"\"\"No Partition: save the result using overwrite or append.\"\"\"\n        print('Saving delta {} with no partition...'.format(write_mode))\n        df.write.format('delta') \\\n            .option(\"overwriteSchema\",True) \\\n            .mode(write_mode.lower()) \\\n            .save(delta_path)\n        print('Success: {} with no partition in {}'.format(write_mode, delta_path))\n  except Exception as ex:\n    errors.append('Error in save_delta_format method: {}'.format(ex))\n    print(ex)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read and save files","showTitle":true,"inputWidgets":{},"nuid":"2ba20f7c-99de-4d86-9aef-0ac730da3df1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["counter = 1\n\nfor dataset_x in source_datasets_son:\n  \"\"\"get adls values\"\"\"\n  storage_access_key_secret_name, storage_account_name, container_name = get_global_values(source_datasets_son[dataset_x][\"sourceModule\"])\n  \n  \"\"\"read parquet or delta source\"\"\"\n  df = read_source_data(scope_name, storage_access_key_secret_name, storage_account_name, container_name, source_datasets_son[dataset_x][\"sourcePath\"], source_datasets_son[dataset_x][\"sourceFileFormat\"])\n\n  \"\"\"create temp view\"\"\"\n  df.createOrReplaceTempView(\"dataset{}\".format(counter))\n  \n  \"\"\"clean memory (disk and cache)\"\"\"\n  df.unpersist()\n  \n  \"\"\"increase counter\"\"\"\n  counter += 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"*Load SourceDatasets and create temporal views","showTitle":true,"inputWidgets":{},"nuid":"e639282a-aab6-4c00-8ec8-f05b1390ddb3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["try:\n  \"\"\"\"execute functions dynamically\"\"\"\n  for function_name in input_parameters[\"functions\"]:  \n    df = locals()[function_name](df, function_name, input_parameters, DataTransformationParameters)\n  \n  finalschema=save_to_delta_format(df,'datatransformation',storage_account_name, output_path, write_mode, input_parameters, dbname= 'dataransformation',DeltaTableName='ComplexTransformations')\n\nexcept Exception as ex:\n    errors.append('Error in main for statement: {}'.format(ex))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Main","showTitle":true,"inputWidgets":{},"nuid":"753f153e-6007-4cd9-a7ea-49a83deddc73"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["counter = 1\nfor dataset_x in source_datasets_son:\n  spark.catalog.dropTempView(\"dataset{}\".format(counter))  \n  counter += 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"*Delete temporal views","showTitle":true,"inputWidgets":{},"nuid":"d53777c4-7e4f-4b0e-a05b-bdac36b74b52"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["if len(errors) > 0:\n  raise Exception(str(errors))\nnotebook_output = {'Errors':'. \\n'.join(errors)}\ndbutils.notebook.exit(finalschema)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Output","showTitle":true,"inputWidgets":{},"nuid":"0d3e0f47-0a37-4060-ab69-fc481383b0c6"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"NB_02_ComplexTransformation","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{"SinkGlobalParameters":{"nuid":"8da05484-f5b8-4256-84f9-8f27af7e48f0","currentValue":"{ \"kv_scope_name\": \"mdmf-scope-dev\", \"kv_workspace_id\": \"mdmf-workspace-id\", \"kv_workspace_pk\": \"mdmf-workspace-pk\", \"ing_sink_storage_name\": \"mdmfadlsdev\", \"ing_sink_storage_secret_name\": \"MDMF-SinkStorageAccount-Key\", \"ing_sink_storage_type\": \"ADLS\", \"ing_sink_container_name\": \"sink\", \"dv_sink_storage_name\": \"mdmfadlsdev\", \"dv_sink_storage_secret_name\": \"MDMF-SinkStorageAccount-Key\", \"dv_sink_container_name\": \"sink\", \"dv_schema_container_name\": \"schemas\", \"dt_sink_storage_name\": \"mdmfadlsdev\", \"dt_sink_storage_secret_name\": \"MDMF-SinkStorageAccount-Key\", \"dt_sink_container_name\": \"datatransformation\" }","widgetInfo":{"widgetType":"text","name":"SinkGlobalParameters","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"DataTransformationParameters":{"nuid":"e20e8c61-083d-446d-9a67-470737588ed0","currentValue":"{       \"FunctionName\":\"ComplexTransformation\",    \"NotebookPath\":\"/Shared/Metadata Driven Ingestion Framework/DataTransformation/NB_02_ComplexTransformation\",    \"DtMethod\":\"Databricks\",    \"OutputPath\":\"Sales1\",    \"InputParameters\":\"{\\r\\n\\t\\\"deltaKeyColumns\\\": [\\\"SalesOrderID\\\",\\\"SalesOrderDetailID\\\",\\\"ProductID\\\"],\\r\\n\\t\\\"deltaPartitionColumns\\\":[\\\"SalesOrderID\\\",\\\"SalesOrderDetailID\\\",\\\"ProductID\\\"],\\r\\n\\t\\\"deltaOutputColumns\\\": [\\\"SalesOrderID\\\",\\\"SalesOrderDetailID\\\",\\\"ProductID\\\",\\\"SalesOrderNumber\\\",\\\"PurchaseOrderNumber\\\",\\\"AccountNumber\\\",\\\"ProductNumber\\\",\\\"Name\\\"],\\r\\n\\t\\\"functions\\\": {\\r\\n\\t\\t\\\"joinDatasets\\\": {\\r\\n\\t\\t\\t\\\"Query\\\": \\\"SELECT d1.SalesOrderID,d1.RevisionNumber,d1.OrderDate,d1.DueDate,d1.ShipDate,d1.Status,d1.OnlineOrderFlag,d1.SalesOrderNumber,d1.PurchaseOrderNumber,d1.AccountNumber,d1.CustomerID,d1.ShipToAddressID,d1.BillToAddressID,d1.ShipMethod,d1.CreditCardApprovalCode,d1.SubTotal,d1.TaxAmt,d1.Freight,d1.TotalDue,d1.Comment,d1.ModifiedDate as SaleOrderHeader_ModifiedDate,d2.SalesOrderDetailID,d2.OrderQty,d2.UnitPrice,d2.UnitPriceDiscount,d2.LineTotal,d2.ModifiedDate as SalesOrderDetail_ModifiedDate,d3.ProductID,d3.ProductNumber,d3.Color,d3.Name,d3.StandardCost,d3.ListPrice,d3.Size,d3.Weight,d3.ProductCategoryID FROM dataset1 d1 INNER JOIN dataset2 d2 ON d1.SalesOrderId = d2.SalesOrderId INNER JOIN dataset3 d3 ON d3.ProductID = d2.ProductID\\\"\\r\\n\\t\\t}\\r\\n\\t}\\r\\n}\",    \"WriteMode\":\"Overwrite\",    \"WriteModeId\":0,    \"SourceDatasets\":\"{ \\\"sourceDatasets\\\": { \\\"dataset1\\\": { \\\"sourceModule\\\": \\\"Ingestion\\\", \\\"sourcePath\\\": \\\"Converted/SQL/AdventureWorks/SalesLT/SalesOrderHeader/2022/04/08/*.parquet\\\", \\\"sourceFileFormat\\\": \\\"Parquet\\\" },\\\"dataset2\\\": { \\\"sourceModule\\\": \\\"Ingestion\\\", \\\"sourcePath\\\": \\\"Converted/SQL/AdventureWorks/SalesLT/SalesOrderDetail/2022/04/08/*.parquet\\\", \\\"sourceFileFormat\\\": \\\"Parquet\\\" },\\\"dataset3\\\": { \\\"sourceModule\\\": \\\"Data Transformation\\\", \\\"sourcePath\\\": \\\"Product\\\", \\\"sourceFileFormat\\\": \\\"Delta\\\" }} }\" }","widgetInfo":{"widgetType":"text","name":"DataTransformationParameters","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":46311312444477}},"nbformat":4,"nbformat_minor":0}
